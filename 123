
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.ResultSetMetaData;
import java.sql.Statement;
import java.util.Properties;

public class PgTableToKafka {
    private static final String KAFKA_TOPIC = "your_topic";
    private static final String KAFKA_BOOTSTRAP_SERVERS = "localhost:9092";

    private static final String POSTGRES_URL = "jdbc:postgresql://localhost:5432/your_database";
    private static final String POSTGRES_USER = "your_user";
    private static final String POSTGRES_PASSWORD = "your_password";
    private static final String TARGET_TABLE = "your_table";

    public static void main(String[] args) {
        // 设置Kafka生产者属性
        Properties kafkaProps = new Properties();
        kafkaProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_BOOTSTRAP_SERVERS);
        kafkaProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        kafkaProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        try (KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProps);
             Connection conn = DriverManager.getConnection(POSTGRES_URL, POSTGRES_USER, POSTGRES_PASSWORD);
             Statement stmt = conn.createStatement()) {

            // 查询表结构
            String query = "SELECT * FROM " + TARGET_TABLE + " LIMIT 1"; // 只查询一行数据获取元数据
            ResultSet rs = stmt.executeQuery(query);
            ResultSetMetaData metaData = rs.getMetaData();

            // 构建表结构的JSON字符串
            StringBuilder schemaInfo = new StringBuilder();
            schemaInfo.append("{ \"table\": \"" + TARGET_TABLE + "\", \"columns\": [");

            for (int i = 1; i <= metaData.getColumnCount(); i++) {
                schemaInfo.append("{");
                schemaInfo.append("\"name\": \"").append(metaData.getColumnName(i)).append("\", ");
                schemaInfo.append("\"type\": \"").append(metaData.getColumnTypeName(i)).append("\"");
                schemaInfo.append("}");
                if (i < metaData.getColumnCount()) {
                    schemaInfo.append(", ");
                }
            }
            schemaInfo.append("] }");

            // 将表结构推送到 Kafka
            ProducerRecord<String, String> record = new ProducerRecord<>(KAFKA_TOPIC, schemaInfo.toString());
            producer.send(record);
            System.out.println("Schema info sent to Kafka topic '" + KAFKA_TOPIC + "': " + schemaInfo);

        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}

WITH not_close_detail AS (
    SELECT 
        td_order_id,
        to_status,
        operate_datetime,
        label
    FROM 
        some_table  -- 在这里放置 not_close_detail 表来源
),
temp_view AS (
    SELECT 
        t1.td_order_id AS busi_no,
        t1.title,
        t1.issue_desc,
        t1.status,
        t1.td_type,
        biz_domain,
        t1.request_user_id AS user_by,
        CASE
            WHEN COALESCE(d.l1_dept_code, '-1') = COALESCE(d.l0_dept_code, '-1')
            THEN '-1' ELSE COALESCE(d.l1_dept_code, '-1')
        END AS l1_dept_code,
        CASE
            WHEN COALESCE(d.l1_dept_cn_name, '异常部门') = COALESCE(d.l0_dept_cn_name, '异常部门')
            THEN '异常部门' ELSE d.l1_dept_cn_name
        END AS l1_dept_cn_name,
        CASE
            WHEN COALESCE(d.l2_dept_code, '-1') = COALESCE(d.l1_dept_code, '-1')
            THEN '-1' ELSE COALESCE(d.l2_dept_code, '-1')
        END AS l2_dept_code,
        CASE
            WHEN COALESCE(d.l2_dept_cn_name, '异常部门') = COALESCE(d.l1_dept_cn_name, '异常部门')
            THEN '异常部门' ELSE COALESCE(d.l2_dept_cn_name, '异常部门')
        END AS l2_dept_cn_name,
        CASE
            WHEN COALESCE(d.l3_dept_code, '-1') = COALESCE(d.l2_dept_code, '-1')
            THEN '-1' ELSE COALESCE(d.l3_dept_code, '-1')
        END AS l3_dept_code,
        CASE
            WHEN COALESCE(d.l3_dept_cn_name, '异常部门') = COALESCE(d.l2_dept_cn_name, '异常部门')
            THEN '异常部门' ELSE COALESCE(d.l3_dept_cn_name, '异常部门')
        END AS l3_dept_cn_name,
        CASE
            WHEN COALESCE(d.l4_dept_code, '-1') = COALESCE(d.l3_dept_code, '-1')
            THEN '-1' ELSE COALESCE(d.l4_dept_code, '-1')
        END AS l4_dept_code,
        CASE
            WHEN COALESCE(d.l4_dept_cn_name, '异常部门') = COALESCE(d.l3_dept_cn_name, '异常部门')
            THEN '异常部门' ELSE COALESCE(d.l4_dept_cn_name, '异常部门')
        END AS l4_dept_cn_name,
        CASE
            WHEN COALESCE(d.l5_dept_code, '-1') = COALESCE(d.l4_dept_code, '-1')
            THEN '-1' ELSE COALESCE(d.l5_dept_code, '-1')
        END AS l5_dept_code,
        CASE
            WHEN COALESCE(d.l5_dept_cn_name, '异常部门') = COALESCE(d.l4_dept_cn_name, '异常部门')
            THEN '异常部门' ELSE COALESCE(d.l5_dept_cn_name, '异常部门')
        END AS l5_dept_cn_name,
        CASE
            WHEN COALESCE(d.l6_dept_code, '-1') = COALESCE(d.l5_dept_code, '-1')
            THEN '-1' ELSE COALESCE(d.l6_dept_code, '-1')
        END AS l6_dept_code,
        CASE
            WHEN COALESCE(d.l6_dept_cn_name, '异常部门') = COALESCE(d.l5_dept_cn_name, '异常部门')
            THEN '异常部门' ELSE COALESCE(d.l6_dept_cn_name, '异常部门')
        END AS l6_dept_cn_name,
        t2.dimension_code AS product_code,
        t2.dimension_cn_name AS product_name,
        CASE
            WHEN COALESCE(t2.prod_fm_code, '-1') = COALESCE(t2.parent_code, '-1')
            THEN NULL ELSE t2.prod_fm_code
        END AS spdt_code,
        CASE
            WHEN COALESCE(t2.prod_fm_code, '-1') = COALESCE(t2.parent_code, '-1')
            THEN NULL ELSE t2.prod_fm_cn_name
        END AS spdt_name,
        CASE
            WHEN COALESCE(t2.prod_code, '-1') = COALESCE(t2.prod_fm_code, '-1')
            THEN NULL ELSE t2.prod_code
        END AS pdt_code,
        CASE
            WHEN COALESCE(t2.prod_code, '-1') = COALESCE(t2.prod_fm_code, '-1')
            THEN NULL ELSE t2.prod_cn_name
        END AS pdt_name,
        CASE
            WHEN COALESCE(t2.sub_prod_code, '-1') = COALESCE(t2.prod_code, '-1')
            THEN NULL ELSE t2.sub_prod_code
        END AS sub_product_code,
        CASE
            WHEN COALESCE(t2.sub_prod_code, '-1') = COALESCE(t2.prod_code, '-1')
            THEN NULL ELSE t2.sub_prod_cn_name
        END AS sub_product_name,
        CASE
            WHEN COALESCE(t2.app_module_id, '-1') = COALESCE(t2.sub_prod_code, '-1')
            THEN NULL ELSE t2.app_module_id
        END AS module_code,
        CASE
            WHEN COALESCE(t2.app_module_id, '-1') = COALESCE(t2.sub_prod_code, '-1')
            THEN NULL ELSE t2.app_module_cn_name
        END AS module_cn_name,
        CASE
            WHEN COALESCE(t2.app_id, '-1') = COALESCE(t2.app_module_id, '-1')
            THEN NULL ELSE t2.app_id
        END AS app_id,
        CASE
            WHEN COALESCE(t2.app_id, '-1') = COALESCE(t2.app_module_id, '-1')
            THEN NULL ELSE t2.app_cn_name
        END AS app_cn_name,
        t1.created_by,
        t1.curr_processor AS curr_process_user,
        t1.opened_by AS opened_user,
        t1.open_datetime AS open_time,
        NULL AS deal_time,
        t1.upgrade_cnt - 1 AS sla_upgrade_cnt,
        CASE
            WHEN t1.sla_status >= 2 THEN 1 ELSE 0
        END AS sla_break_flag,
        CASE
            WHEN t1.sla_status = '11' THEN '分析中'
        END AS respond_sla_status,
        CASE
            WHEN t1.sla_status = '2' THEN '处理中'
        END AS closed_sla_status,
        ROUND((UNIX_TIMESTAMP(CURRENT_TIMESTAMP) - UNIX_TIMESTAMP(t1.open_datetime))/(60*60*24), 2) AS saty_time,
        t1.related_service_order AS relate_order_no,
        NULL AS sla_warn,
        CASE
            WHEN t1.status IN ('11', '2') THEN FROM_UNIXTIME(UNIX_TIMESTAMP(t1.status_sla_break_datetime_only) + 8*60*60, 'yyyy-MM-dd HH:mm:ss')
        END AS sla_time,
        t1.to_do_person,
        t1.to_do_applicant,
        t1.tmp_trace_person AS trace_person,
        t1.tmp_trace_upgrade AS trace_upgrade,
        t1.operate_datetime,
        DENSE_RANK() OVER (PARTITION BY t1.td_order_id ORDER BY t3.label DESC) AS parent_rn,
        ROW_NUMBER() OVER (PARTITION BY t1.td_order_id, t3.label ORDER BY t3.operate_datetime ASC) AS sub_rn,
        t1.relate_user,
        t1.related_service_order,
        t1.sla_status
    FROM 
        temp t1
    LEFT JOIN 
        ipd_ops_dim.dim_pub_rels_emp_dept_t d
    ON 
        LOWER(t1.created_by) = LOWER(d.w3_account)
    LEFT JOIN 
        ipd_ops_dim.dim_pub_ci_ciinfo ci  -- 获取 app_id
    ON 
        t1.request_ci_id = ci.ci_id
    LEFT JOIN 
        (
            SELECT 
                service_id,
                CASE WHEN COALESCE(TRIM(request_ci_id), '') = '' THEN NULL ELSE request_ci_id END AS request_ci_id,
                CASE WHEN COALESCE(TRIM(ci_module_id), '') = '' THEN NULL ELSE ci_module_id END AS ci_module_id
            FROM 
                ipd_ops_dwi.dwi_v2_t_importal_service_request_hi
        ) sd
    ON 
        t1.related_service_order = sd.service_id
    LEFT JOIN 
        ipd_ops_dim.dim_pub_ci_ciinfo t4
    ON 
        sd.request_ci_id = t4.ci_id
    LEFT JOIN 
        ipd_ops_dim.dim_pub_dimension_ipmt t2
    ON 
        COALESCE(ci.app


<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.huawei.scs</groupId>
    <artifactId>scs-flink-sdk-original-api</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <scala.version>2.11</scala.version>
        <flink.version>1.12.2-hw-ei-312010</flink.version>
        <flink.kafka.version>1.12.0</flink.kafka.version>
        <scope>provided</scope>
    </properties>

    <dependencies>
        <!--确保该包位于dependences的最前面,防止冲突-->
        <dependency>
            <groupId>com.huawei.scs</groupId>
            <artifactId>scs-common-flink-v112</artifactId>
            <version>1.2-RELEASE</version>
        </dependency>
        <!-- Use this dependency if you are using the DataStream API -->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java_${scala.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-scala_${scala.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${scope}</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka_${scala.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${scope}</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-jdbc_${scala.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${scope}</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients_${scala.version}</artifactId>
            <version>${flink.version}</version>
            <scope>${scope}</scope>
        </dependency>

        <!--公共包-->
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <version>1.2.3</version>
        </dependency>

        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <version>1.2.3</version>
        </dependency>

        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-access</artifactId>
            <version>1.2.3</version>
        </dependency>
        <dependency>
            <groupId>net.logstash.logback</groupId>
            <artifactId>logstash-logback-encoder</artifactId>
            <version>6.3</version>
            <scope>runtime</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java-bridge_2.11</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner_2.11</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner-blink_2.11</artifactId>
            <version>${flink.version}</version>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-json</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.postgresql</groupId>
            <artifactId>postgresql</artifactId>
            <version>42.5.1</version>
        </dependency>

    </dependencies>

    <build>

        <plugins>
            <!-- Maven Assembly Plugin -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>2.6</version>
                <configuration>
                    <!-- get all project dependencies -->
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <!-- bind to the packaging phase -->
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
