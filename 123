
package com.helei.tradeapplication.service.impl;


import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeType;
import com.helei.dto.account.UserAccountInfo;
import com.helei.tradeapplication.dto.GroupOrder;
import com.helei.dto.trade.TradeSignal;
import com.helei.interfaces.CompleteInvocation;
import com.helei.tradeapplication.manager.ExecutorServiceManager;
import com.helei.tradeapplication.service.OrderService;
import com.helei.tradeapplication.service.TradeSignalService;
import com.helei.tradeapplication.service.UserAccountInfoService;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;


/**
 * 处理交易信号
 */
@Slf4j
@Service
public class KafkaTradeSignalService implements TradeSignalService {

    private final ExecutorService executor;

    @Autowired
    private UserAccountInfoService userAccountInfoService;

    @Autowired
    private OrderService orderService;


    public KafkaTradeSignalService(ExecutorServiceManager executorServiceManager) {
        this.executor = executorServiceManager.getTradeExecutor();
    }


    /**
     * 处理交易信号
     *
     * @param runEnv    运行环境
     * @param tradeType 交易类型
     * @param signal    信号
     * @return true 无论处理结果如何都忽略到改信号
     */
    public boolean resolveTradeSignal(RunEnv runEnv, TradeType tradeType, TradeSignal signal) {

        try {
            userAccountInfoService
                    // 1 查询环境下的账户
                    .queryEnvAccountInfo(runEnv, tradeType)
                    // 2 生成订单并交易
                    .thenApplyAsync(accounts -> makeOrdersAndSend2Trade(runEnv, tradeType, signal, accounts), executor);
        } catch (Exception e) {
            log.error("处理信号[{}]时发生错误", signal, e);
        }

        return true;
    }


    /**
     * 构建订单，符合条件的提交到交易.
     * <p>并不会真正把订单提交到交易所，而是写入数据库后，再写入kafka的topic里</p>
     *
     * @param runEnv       runEnv
     * @param tradeType    tradeType
     * @param signal       signal
     * @param accountInfos accountInfos
     * @return List<BaseOrder>
     */
    private List<GroupOrder> makeOrdersAndSend2Trade(RunEnv runEnv, TradeType tradeType, TradeSignal signal, List<UserAccountInfo> accountInfos) {

        List<CompletableFuture<GroupOrder>> futures = new ArrayList<>();

        for (UserAccountInfo accountInfo : accountInfos) {

            //Step 1 过滤掉账户设置不接受此信号的
            if (filterAccount(signal, accountInfo)) {
                log.warn("accountId[{}]不能执行信号 [{}]", accountInfo.getId(), signal);
            }

            CompletableFuture<GroupOrder> future = userAccountInfoService
                    //Step 2 查询实时的账户数据
                    .queryAccountRTInfo(runEnv, tradeType, accountInfo.getId())
                    //Step 3 生产订单
                    .thenApplyAsync(accountRTData -> {
                        final GroupOrder[] groupOrder = {null};

                        try {
                            CountDownLatch latch = new CountDownLatch(1);

                            orderService.makeOrder(accountInfo, accountRTData, signal, new CompleteInvocation<>() {
                                @Override
                                public void success(GroupOrder order) {
                                    groupOrder[0] = order;
                                    log.info("创建订单[{}]成功", order);
                                }

                                @Override
                                public void fail(GroupOrder order, String errorMsg) {
                                    groupOrder[0] = order;
                                    log.info("创建订单失败[{}],错误原因[{}]", order, errorMsg);
                                }

                                @Override
                                public void finish() {
                                    latch.countDown();
                                }
                            });

                            //等待订单创建完成
                            latch.await();

                        } catch (Exception e) {
                            log.error("为accountId[{}]创建订单时出错, signal[{}]", accountInfo.getId(), signal, e);
                        }
                        return groupOrder[0];
                    })
                    .exceptionallyAsync(throwable -> {
                        if (throwable != null) {
                            log.error("创建订单时发生错误", throwable);
                        }
                        return null;
                    });

            futures.add(future);
        }


        //等待执行完成
        List<GroupOrder> groupOrders = new ArrayList<>();
        for (CompletableFuture<GroupOrder> future : futures) {
            try {
                GroupOrder order = future.get();
                groupOrders.add(order);
            } catch (ExecutionException | InterruptedException e) {
                log.error("获取订单结果处理订单结果出错", e);
                throw new RuntimeException(e);
            }
        }
        return groupOrders;
    }


    /**
     * 根据账户设置过滤
     *
     * @param signal  信号
     * @param account 账户
     * @return List<UserAccountInfo>
     */
    private boolean filterAccount(TradeSignal signal, UserAccountInfo account) {
        return !account.getUsable().get() || !account.getSubscribeSymbol().contains(signal.getSymbol());
    }
}





package com.helei.tradeapplication.service.impl;

import com.alibaba.fastjson.JSONObject;
import com.helei.constants.CEXType;
import com.helei.constants.order.GroupOrderStatus;
import com.helei.constants.order.OrderEvent;
import com.helei.constants.order.OrderStatus;
import com.helei.constants.order.OrderType;
import com.helei.constants.trade.TradeType;
import com.helei.dto.account.AccountPositionConfig;
import com.helei.dto.order.BaseOrder;
import com.helei.dto.order.CEXTradeOrder;
import com.helei.dto.order.CEXTradeOrderWrap;
import com.helei.tradeapplication.dto.GroupOrder;
import com.helei.dto.account.AccountRTData;
import com.helei.dto.account.UserAccountInfo;
import com.helei.dto.trade.TradeSignal;
import com.helei.interfaces.CompleteInvocation;
import com.helei.tradeapplication.manager.ExecutorServiceManager;
import com.helei.tradeapplication.service.IBinanceContractOrderService;
import com.helei.tradeapplication.service.OrderEventProcessService;
import com.helei.tradeapplication.supporter.TradeOrderBuildSupporter;
import com.helei.util.KafkaUtil;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.List;
import java.util.concurrent.ExecutionException;


@Slf4j
@Service
public class OrderServiceImpl extends OrderEventProcessService {


    @Autowired
    private KafkaProducerService kafkaProducerService;


    @Autowired
    private IBinanceContractOrderService binanceContractOrderService;


    @Autowired
    private TradeOrderBuildSupporter tradeOrderBuildSupporter;


    @Autowired
    public OrderServiceImpl(ExecutorServiceManager executorServiceManager) {
        super(executorServiceManager.getOrderExecutor());
        super.startProcessEvents();
    }


    /**
     * 生成订单
     *
     * @param accountInfo   账户信息
     * @param accountRTData 账户实时数据
     * @param signal        信号
     */
    @Override
    public void makeOrder(UserAccountInfo accountInfo, AccountRTData accountRTData, TradeSignal signal, CompleteInvocation<GroupOrder> invocation) {

        AccountPositionConfig accountPositionConfig = accountInfo.getAccountPositionConfig();
        OrderType orderType = accountPositionConfig.getOrderType();

        GroupOrder groupOrder = new GroupOrder();

        //Step 1 创建主单
        CEXTradeOrderWrap orderWrapper = switch (orderType) {
            case LIMIT -> tradeOrderBuildSupporter.buildLimitOrder(accountInfo, accountRTData, signal);
            case MARKET -> tradeOrderBuildSupporter.buildMarketOrder(accountInfo, accountRTData, signal);
            default -> null;
        };


        long userId = accountInfo.getId();
        long accountId = accountRTData.getAccountId();

        if (orderWrapper == null) {
            log.warn("userId[{}]-accountId[{}]创建主订单结果为null, signalId[{}]", userId, accountId, signal.getId());
            return;
        }

        //获取订单
        CEXTradeOrder order = orderWrapper.getFullFieldOrder();

        // 资金不足
        if (order.getQuantity().doubleValue() <= 0) {
            order.setStatus(OrderStatus.BALANCE_INSUFFICIENT);
        }
        groupOrder.setMainOrder(order);


        // 满足资金不足的标记，给group order也打上，不继续创建止盈止损单了
        if (OrderStatus.BALANCE_INSUFFICIENT.equals(order.getStatus())) {
            log.warn("userId[{}]-account[{}] 资金不足，将不会提交订单[{}]到交易所, signalId[{}]", userId, accountId, order.getOrderId(), signal.getId());
            groupOrder.setGroupOrderStatus(GroupOrderStatus.BALANCE_INSUFFICIENT);

            super.submitOrderEvent(groupOrder, OrderEvent.BALANCE_INSUFFICIENT, invocation);
            return;
        }

        //Step 2 根据策略创建止损、止盈单


        log.info("订单[{}]创建成功", order);
        //Step 3 提交订单创建事件
        super.submitOrderEvent(groupOrder, OrderEvent.CREATED_ORDER, invocation);
    }


    @Override
    public GroupOrder writeOrder2Kafka(GroupOrder order) throws ExecutionException, InterruptedException {
        //Step 1 从主单中取出环境信息，生成topic
        BaseOrder mainOrder = order.getMainOrder();

        String topic = KafkaUtil.getOrderSymbolTopic(mainOrder.getRunEnv(), mainOrder.getTradeType(), mainOrder.getSymbol());

        //Step 2 获取交易所订单列表发送
        List<CEXTradeOrder> cexTradeOrders = order.getCexTradeOrders();

        kafkaProducerService.sendMessage(topic, JSONObject.toJSONString(cexTradeOrders)).get();

        log.debug("订单order[{}]写入kafka成功", order);

        return order;
    }


    @Override
    public GroupOrder writeOrder2DB(GroupOrder order) {

        BaseOrder mainOrder = order.getMainOrder();
        CEXType cexType = mainOrder.getCexType();
        TradeType tradeType = mainOrder.getTradeType();


        if (CEXType.BINANCE.equals(cexType) && TradeType.CONTRACT.equals(tradeType)) {
            List<CEXTradeOrder> cexTradeOrders = binanceContractOrderService.saveGroupOrder(order);
            order.setCexTradeOrders(cexTradeOrders);
            return order;
        }

        return null;
    }


}





package com.helei.tradeapplication.service.impl;

import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeType;
import com.helei.dto.account.AccountRTData;
import com.helei.dto.account.UserAccountInfo;
import com.helei.tradeapplication.cache.UserInfoCache;
import com.helei.tradeapplication.manager.ExecutorServiceManager;
import com.helei.tradeapplication.service.UserAccountInfoService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;

import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;

@Component
public class UserAccountInfoServiceImpl implements UserAccountInfoService {

    private final ExecutorService executor;

    @Autowired
    private UserInfoCache userInfoCache;

    public UserAccountInfoServiceImpl(ExecutorServiceManager executorServiceManager) {
        this.executor = executorServiceManager.getQueryExecutor();
    }

    @Override
    public CompletableFuture<List<UserAccountInfo>> queryEnvAccountInfo(RunEnv env, TradeType tradeType) {


        return null;
    }

    @Override
    public CompletableFuture<AccountRTData> queryAccountRTInfo(RunEnv env, TradeType tradeType, long accountId) {
        return CompletableFuture.supplyAsync(()->userInfoCache.queryAccountRTData(env, tradeType, accountId), executor);
    }

}


package com.helei.tradeapplication.service;


import com.baomidou.mybatisplus.extension.service.IService;
import com.helei.tradeapplication.dto.GroupOrder;
import com.helei.dto.order.CEXTradeOrder;

import java.util.List;

/**
 * <p>
 * 币安合约交易订单表 服务类
 * </p>
 *
 * @author com.helei
 * @since 2024-11-05
 */
public interface IBinanceContractOrderService extends IService<CEXTradeOrder> {


    /**
     * 保存订单
     * @param order 订单
     */
    List<CEXTradeOrder> saveGroupOrder(GroupOrder order);

}



package com.helei.tradeapplication.service;

import com.helei.constants.order.OrderEvent;
import com.helei.constants.order.GroupOrderStatus;
import com.helei.tradeapplication.dto.GroupOrder;
import com.helei.interfaces.CompleteInvocation;
import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.extern.slf4j.Slf4j;

import java.util.concurrent.*;


@Slf4j
public abstract class OrderEventProcessService implements OrderService {


    /**
     * 写入db的重试次数
     */
    private static final int WRITE_DB_RETRY_TIMES = 3;

    /**
     * 写入kafka的重试次数
     */
    private static final int WRITE_KAFKA_RETRY_TIMES = 3;


    /**
     * 阻塞队列， 用于存放订单和当前订单的事件
     */
    private final BlockingQueue<OrderProcessTask> eventQueue = new LinkedBlockingQueue<>();


    /**
     * 存放订单回调的map
     */
    private final ConcurrentMap<GroupOrder, CompleteInvocation<GroupOrder>> invocationMap = new ConcurrentHashMap<>();


    /**
     * 记录重试次数的map
     */
    private final ConcurrentMap<GroupOrder, Integer> retryMap = new ConcurrentHashMap<>();


    /**
     * 执行的线程池
     */
    private final ExecutorService executor;


    public OrderEventProcessService(ExecutorService executor) {
        this.executor = executor;
    }


    /**
     * 提交订单事件
     *
     * @param order              订单
     * @param event              订单事件
     * @param completeInvocation 完成的回调函数
     */
    public void submitOrderEvent(GroupOrder order, OrderEvent event, CompleteInvocation<GroupOrder> completeInvocation) {
        invocationMap.compute(order, (k, v) -> {
            submitOrderEvent(order, event);
            return completeInvocation;
        });
    }

    /**
     * 提交订单事件
     *
     * @param order 订单
     * @param event 订单事件
     */
    public void submitOrderEvent(GroupOrder order, OrderEvent event) {
        try {
            eventQueue.put(new OrderProcessTask(order, event));
        } catch (InterruptedException e) {
            log.error("提交订单[{}]事件[{}]失败", order, event, e);
            throw new RuntimeException("提交订单事件失败", e);
        }
    }

    /**
     * 事件处理
     *
     * @param order 订单
     * @param event 事件
     */
    public void processOrderEvent(GroupOrder order, OrderEvent event) {
        log.debug("开始处理订单[{}]的事件[{}]", order, event);

        OrderEvent next = switch (event) {
            case CREATED_ORDER -> createdOrderProcess(order);

            case SEND_TO_DB -> sendToDBProcess(order);
            case SEND_TO_KAFKA -> sendToKafkaProcess(order);

            case SEND_TO_DB_RETRY -> sendToDBRetryProcess(order);
            case SEND_TO_KAFKA_RETRY -> sendToKafkaRetryProcess(order);

            case SEND_TO_DB_FINAL_ERROR -> errorProcess(order, OrderEvent.SEND_TO_DB_FINAL_ERROR);
            case SEND_TO_KAFKA_FINAL_ERROR -> errorProcess(order, OrderEvent.SEND_TO_KAFKA_FINAL_ERROR);
            case UN_SUPPORT_EVENT_ERROR -> errorProcess(order, OrderEvent.UN_SUPPORT_EVENT_ERROR);

            case BALANCE_INSUFFICIENT -> balanceInsufficientProcess(order);

            case COMPLETE -> successProcess(order);
            case ERROR -> errorProcess(order, OrderEvent.ERROR);
            case CANCEL -> cancelProcess(order);
        };

        if (next != null) {
            submitOrderEvent(order, next);
        }

        log.debug("订单[{}]的事件[{}]处理完毕", order, event);
    }


    /**
     * 资金不足的订单处理
     *
     * @param order groupOrder
     * @return 下一个事件
     */
    private OrderEvent balanceInsufficientProcess(GroupOrder order) {
        if (GroupOrderStatus.BALANCE_INSUFFICIENT.equals(order.getGroupOrderStatus())) {
            try {
                //写入数据库
                writeOrder2DB(order);
            } catch (Exception e) {
                //重试
                return OrderEvent.SEND_TO_DB_RETRY;
            }
            //错误事件
            errorProcess(order, OrderEvent.BALANCE_INSUFFICIENT);
        }
        return OrderEvent.UN_SUPPORT_EVENT_ERROR;
    }


    /**
     * 取消订单
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent cancelProcess(GroupOrder order) {
        //TODO 取消订单逻辑，未写入kafka的标记就好，写入kafka的还需要向另外的kafka里写上取消的消息，订单提交服务收到后进行取消
        return null;
    }


    /**
     * 执行成功的事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent successProcess(GroupOrder order) {

        CompleteInvocation<GroupOrder> invocation = invocationMap.remove(order);

        if (invocation != null) {
            invocation.success(order);
            invocation.finish();
        }

        return null;
    }


    /**
     * 错误事件处理
     *
     * @param order order
     * @param event 时间
     * @return 下一个事件
     */
    private OrderEvent errorProcess(GroupOrder order, OrderEvent event) {

        CompleteInvocation<GroupOrder> invocation = invocationMap.remove(order);

        if (invocation != null) {
            invocation.fail(order, event.name());
            invocation.finish();
        }

        return null;
    }


    /**
     * 发送到kafka错误重试事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent sendToKafkaRetryProcess(GroupOrder order) {
        if (GroupOrderStatus.WRITE_IN_KAFKA.equals(order.getGroupOrderStatus())) {
            Integer times = retryMap.remove(order);
            times = times == null ? 0 : times;

            //超过重试次数
            if (times > WRITE_KAFKA_RETRY_TIMES) {
                return OrderEvent.SEND_TO_KAFKA_FINAL_ERROR;
            }


            try {
                GroupOrder result = writeOrder2Kafka(order);

                if (result == null) return OrderEvent.CANCEL;

                return OrderEvent.COMPLETE;
            } catch (Exception e) {
                log.error("写入Order[{}]到kafka发生错误,重试次数[{}]", order, times, e);
                retryMap.put(order, times + 1);
                return OrderEvent.SEND_TO_KAFKA_RETRY;
            }
        }

        return OrderEvent.UN_SUPPORT_EVENT_ERROR;
    }


    /**
     * 发送到kafka事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent sendToKafkaProcess(GroupOrder order) {
        if (GroupOrderStatus.WRITE_IN_DB.equals(order.getGroupOrderStatus())) {
            // 发送kafka
            try {
                order.setGroupOrderStatus(GroupOrderStatus.WRITE_IN_KAFKA);

                GroupOrder result = writeOrder2Kafka(order);

                if (result == null) return OrderEvent.CANCEL;
            } catch (Exception e) {
                log.error("写入Order[{}]到kafka发生错误", order, e);
                return OrderEvent.SEND_TO_KAFKA_RETRY;
            }
            return OrderEvent.COMPLETE;
        }
        return OrderEvent.UN_SUPPORT_EVENT_ERROR;
    }


    /**
     * 发送到DB错误重试事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent sendToDBRetryProcess(GroupOrder order) {
        GroupOrderStatus groupOrderStatus = order.getGroupOrderStatus();
        if (GroupOrderStatus.WRITE_IN_DB.equals(groupOrderStatus) || GroupOrderStatus.BALANCE_INSUFFICIENT.equals(groupOrderStatus)) {
            Integer times = retryMap.remove(order);
            times = times == null ? 0 : times;

            //超过重试次数
            if (times > WRITE_DB_RETRY_TIMES) {
                return OrderEvent.SEND_TO_DB_FINAL_ERROR;
            }

            try {
                GroupOrder result = writeOrder2DB(order);

                if (result == null) return OrderEvent.CANCEL;

                //资金不足，只写入数据库记录
                if (GroupOrderStatus.BALANCE_INSUFFICIENT.equals(groupOrderStatus)) return OrderEvent.ERROR;

                return OrderEvent.SEND_TO_KAFKA;
            } catch (Exception e) {
                log.error("写入Order[{}]到数据库发生错误, 重试次数[{}]", order, times, e);
                retryMap.put(order, times + 1);
                return OrderEvent.SEND_TO_DB_RETRY;
            }
        }

        return OrderEvent.UN_SUPPORT_EVENT_ERROR;
    }


    /**
     * 发送到DB事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent sendToDBProcess(GroupOrder order) {
        if (GroupOrderStatus.CREATED.equals(order.getGroupOrderStatus())) {
            // 写数据库
            try {
                order.setGroupOrderStatus(GroupOrderStatus.WRITE_IN_DB);

                GroupOrder result = writeOrder2DB(order);

                if (result == null) return OrderEvent.CANCEL;
            } catch (Exception e) {
                log.error("写入Order[{}]到数据库发生错误", order, e);
                return OrderEvent.SEND_TO_DB_RETRY;
            }
            return OrderEvent.SEND_TO_KAFKA;
        }
        return OrderEvent.UN_SUPPORT_EVENT_ERROR;
    }

    /**
     * 创建订单事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent createdOrderProcess(GroupOrder order) {
        //订单创建事件
        order.setGroupOrderStatus(GroupOrderStatus.CREATED);
        return OrderEvent.SEND_TO_DB;
    }


    /**
     * 开始处理事件
     */
    public void startProcessEvents() {
        while (!eventQueue.isEmpty()) {
            try {
                OrderProcessTask task = eventQueue.take();

                executor.execute(() -> processOrderEvent(task.getOrder(), task.getOrderEvent()));
            } catch (InterruptedException e) {
                log.error("处理事件时发生错误", e);
            }
        }
    }


    /**
     * 订单处理任务，包含订单信息和订单事件
     */
    @Data
    @AllArgsConstructor
    @NoArgsConstructor
    public static class OrderProcessTask {

        /**
         * 订单信息
         */
        private GroupOrder order;

        /**
         * 订单事件
         */
        private OrderEvent orderEvent;
    }


}


package com.helei.tradeapplication.service;

import com.helei.dto.account.AccountRTData;
import com.helei.dto.account.UserAccountInfo;
import com.helei.tradeapplication.dto.GroupOrder;
import com.helei.dto.trade.TradeSignal;
import com.helei.interfaces.CompleteInvocation;

import java.util.concurrent.ExecutionException;

public interface OrderService {


    /**
     * 生成订单
     *
     * @param accountInfo   账户信息
     * @param accountRTData 账户实时数据
     * @param signal        信号
     * @param invocation    生成订单结果的回调
     */
    void makeOrder(UserAccountInfo accountInfo, AccountRTData accountRTData, TradeSignal signal, CompleteInvocation<GroupOrder> invocation);


    /**
     * 将订单写到kafka
     *
     * @param order 订单数据
     * @return 订单数据
     */
    GroupOrder writeOrder2Kafka(GroupOrder order) throws ExecutionException, InterruptedException;


    /**
     * 将订单写入数据库
     *
     * @param order order
     * @return 订单数据
     */
    GroupOrder writeOrder2DB(GroupOrder order);
}


package com.helei.tradeapplication.supporter;

import com.helei.constants.order.OrderType;
import com.helei.constants.order.TimeInForce;
import com.helei.dto.account.AccountPositionConfig;
import com.helei.dto.account.AccountRTData;
import com.helei.dto.account.PositionInfo;
import com.helei.dto.account.UserAccountInfo;
import com.helei.dto.order.BaseOrder;
import com.helei.dto.order.CEXTradeOrder;
import com.helei.dto.order.type.*;
import com.helei.dto.trade.BalanceInfo;
import com.helei.dto.trade.TradeSignal;
import com.helei.snowflack.BRStyle;
import com.helei.snowflack.SnowFlakeFactory;
import com.helei.util.OrderQuantityCalUtil;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;

import java.math.BigDecimal;


/**
 * 构建用于交易的订单，自带id
 */
@Component
public class TradeOrderBuildSupporter {

    @Autowired
    private SnowFlakeFactory snowFlakeFactory;

    /**
     * 构建限价单
     *
     * @param accountInfo   账户信息
     * @param accountRTData 账户实时数据
     * @param signal        信号
     * @return 限价单
     */
    public LimitOrder buildLimitOrder(UserAccountInfo accountInfo, AccountRTData accountRTData, TradeSignal signal) {
        // Step 1 参数检验
        BigDecimal enterPrice = signal.getEnterPrice();
        if (enterPrice == null) return null;

        AccountPositionConfig positionConfig = accountInfo.getAccountPositionConfig();

        // Step 2 创建订单
        String symbol = signal.getSymbol();


        // Step 2.1 基础订单信息
        BaseOrder baseOrder = BaseOrder.builder()
                .orderId(nextId(OrderType.LIMIT))
                .runEnv(accountInfo.getRunEnv())
                .tradeType(accountInfo.getTradeType())
                .cexType(accountInfo.getCexType())
                .symbol(symbol)
                .side(signal.getTradeSide())
                .positionSide(positionConfig.getPositionSide())
                .userId(accountInfo.getUserId())
                .accountId(accountRTData.getAccountId())
                .build();
        LimitOrder limitOrder = new LimitOrder(baseOrder);

        // Step 2.2 订单价格、数量等 LimitOrder 信息
        BalanceInfo balanceInfo = accountRTData.getAccountBalanceInfo().getBalances().get(symbol);
        PositionInfo positionInfo = accountRTData.getAccountPositionInfo().getPositions().get(symbol);


        double quantity = OrderQuantityCalUtil.riskPercentBasedQuantityCalculate(
                balanceInfo.getFree(),
                positionConfig.getRiskPercent(),
                enterPrice.doubleValue(),
                positionInfo.getEnterPosition(),
                positionInfo.getPosition(),
                signal.getStopPrice().doubleValue()
        );

        limitOrder.setTimeInForce(TimeInForce.GTC);
        limitOrder.setPrice(enterPrice);
        limitOrder.setQuantity(BigDecimal.valueOf(quantity));

        return limitOrder;
    }


    /**
     * 构建市价单
     *
     * @param accountInfo   账户信息
     * @param accountRTData 账户实时数据
     * @param signal        信号
     * @return 限价单
     */
    public MarketOrder buildMarketOrder(UserAccountInfo accountInfo, AccountRTData accountRTData, TradeSignal signal) {
        return null;
    }

    /**
     * 构建市价止损单
     *
     * @param accountInfo   账户信息
     * @param accountRTData 账户实时数据
     * @param symbol        交易对
     * @return 限价单
     */
    public StopLossMarketOrder buildStopMarketOrder(UserAccountInfo accountInfo, AccountRTData accountRTData, String symbol) {
        return null;
    }


    /**
     * 构建限价止损单
     *
     * @param accountInfo   账户信息
     * @param accountRTData 账户实时数据
     * @param symbol        交易对
     * @return 限价单
     */
    public StopLossLimitOrder buildStopLimitOrder(UserAccountInfo accountInfo, AccountRTData accountRTData, String symbol) {
        return null;
    }

    /**
     * 构建市价止盈单
     *
     * @param accountInfo   账户信息
     * @param accountRTData 账户实时数据
     * @param symbol        交易对
     * @return 限价单
     */
    public TakeProfitMarketOrder buildTakeProfitMarketOrder(UserAccountInfo accountInfo, AccountRTData accountRTData, String symbol) {
        return null;
    }

    /**
     * 构建限价止盈单
     *
     * @param accountInfo   账户信息
     * @param accountRTData 账户实时数据
     * @param symbol        交易对
     * @return 限价单
     */
    public TakeProfitLimitOrder buildTakeProfitLimitOrder(UserAccountInfo accountInfo, AccountRTData accountRTData, String symbol) {
        return null;
    }

    /**
     * buildTrailingSTIDMarketOrder
     *
     * @param accountInfo   账户信息
     * @param accountRTData 账户实时数据
     * @param symbol        交易对
     * @return 限价单
     */
    public CEXTradeOrder buildTrailingSTIDMarketOrder(UserAccountInfo accountInfo, AccountRTData accountRTData, String symbol) {
        return null;
    }


    /**
     * 获取下一id
     * @param orderType 订单类型
     * @return 订单id
     */
    private String nextId(OrderType orderType) {
        return snowFlakeFactory.nextId(BRStyle.TRADE_SIGNAL, orderType.name());
    }
}


shinano:
  quantity:
    trade_app:
      run_type: # 运行环境配置
        configs:
          - env: NORMAL
            trade_type:
              - SPOT
        snow_flow: # 雪花算法配置
          datacenter_id: 0 # 数据中心id
          machine_id: 1 # 机器id

      kafka:
        bootstrap_servers: 127.0.0.1:9092 # Kafka服务器地址
#        bootstrap_servers: 192.168.1.3:9092 # Kafka服务器地址
        group_id: trade_app_test_group
        kafka_num_partitions: 1
        kafka_replication_factor: 1

      redis:
        # 如果需要密码，格式为 redis://:password@localhost:6379
        #        url: redis://127.0.0.1:6379
        url: redis://192.168.1.3:6379

      signal: # 信号设置
        normal: # 运行环境
          spot: # 交易类型
            - symbol: btcusdt # 交易对名称
              signal_names: # 信号名list
                - test1
                - test2
                - test3
            - symbol: ethusdt
              signal_names:
                - test1
                - test2
                - test3
          contract:
            - symbol: btcusdt
              signal_names:
                - test1
                - test2
                - test3
            - symbol: ethusdt
              signal_names:
                - test1
                - test2
                - test3
        test_net: { }


package com.helei.tradesignalprocess.config;

import com.helei.constants.trade.TradeType;
import com.helei.constants.RunEnv;
import com.helei.dto.config.RunTypeConfig;
import com.helei.dto.kafka.TradeSignalTopic;
import lombok.Data;
import org.yaml.snakeyaml.Yaml;

import java.io.InputStream;
import java.io.Serializable;
import java.util.Map;

@Data
public class TradeSignalConfig implements Serializable {
    private static final String CONFIG_FILE = "trade-signal-config.yaml";

    public static final TradeSignalConfig TRADE_SIGNAL_CONFIG;

    /**
     * 信号名
     */
    private String name;

    /**
     * 信号交易对
     */
    private String symbol;

    /**
     * 运行环境设置
     */
    private RunTypeConfig run_type;

    /**
     * 历史k线加载批大小
     */
    private int historyKLineBatchSize;

    /**
     * 批加载并发度
     */
    private int batchLoadConcurrent;

    /**
     * 实时数据配置
     */
    private RealtimeConfig realtime;

    static {
        Yaml yaml = new Yaml();
        try (InputStream inputStream = TradeSignalConfig.class.getClassLoader().getResourceAsStream(CONFIG_FILE)) {
            if (inputStream == null) {
                throw new IllegalArgumentException("File not found: " + CONFIG_FILE);
            }
            Map<String, Object> yamlData = yaml.load(inputStream);
            Map<String, Object> shinano = (Map<String, Object>) yamlData.get("shinano");
            Map<String, Object> quantity = (Map<String, Object>) shinano.get("quantity");
            Map<String, Object> trade_signal_maker = (Map<String, Object>) quantity.get("trade_signal_maker");


            TRADE_SIGNAL_CONFIG = yaml.loadAs(yaml.dump(trade_signal_maker), TradeSignalConfig.class);

        } catch (Exception e) {
            throw new RuntimeException("Failed to load YAML file: " + CONFIG_FILE, e);
        }
    }

    private TradeSignalConfig() {}

    /**
     * 获取最终交易信号写入到kafka到topic
     * @return topic
     */
    public TradeSignalTopic getSinkTopic() {
        return new TradeSignalTopic(getRun_env(), getTrade_type(), symbol, name);
    }

    public TradeType getTrade_type() {
        return run_type.getConfigs().getFirst().getTrade_type().getFirst();
    }

    public RunEnv getRun_env() {
        return run_type.getConfigs().getFirst().getEnv();
    }


    @Data
    public static class RealtimeConfig  implements Serializable  {

        private RealtimeKafkaConfig kafka;

        private RealtimeFlinkConfig flink;

    }


    @Data
    public static class RealtimeKafkaConfig  implements Serializable  {

        /**
         * 输入的配置
         */
        private KafkaServerConfig input;

        /**
         * 输出的配置
         */
        private KafkaServerConfig output;

    }

    @Data
    public static class KafkaServerConfig  implements Serializable  {
        /**
         * kafka集群连接地址
         */
        private String bootstrapServer;

        /**
         * 消费者组名
         */
        private String groupId;

        /**
         * 事务超时时间，需要比kafka broker 中设置的小
         */
        private String transaction_timeout_ms;
    }

    @Data
    public static class RealtimeFlinkConfig  implements Serializable  {

        /**
         * flink job manager host
         */
        private String jobManagerHost;
        /**
         * flink job manager port
         */
        private Integer jobManagerPort;
    }

    public static void main(String[] args) {
        System.out.println(TRADE_SIGNAL_CONFIG);
    }

}








package com.helei.tradesignalprocess.stream.d_decision;

import com.helei.dto.config.SnowFlowConfig;
import com.helei.dto.trade.IndicatorMap;
import com.helei.dto.trade.IndicatorSignal;
import com.helei.dto.trade.SignalGroupKey;
import com.helei.snowflack.BRStyle;
import com.helei.snowflack.SnowFlakeFactory;
import com.helei.tradesignalprocess.config.TradeSignalConfig;
import lombok.Getter;
import lombok.Setter;
import lombok.extern.slf4j.Slf4j;
import org.apache.flink.api.common.state.MapState;
import org.apache.flink.api.common.state.MapStateDescriptor;
import org.apache.flink.api.common.typeinfo.BasicTypeInfo;
import org.apache.flink.api.common.typeinfo.TypeHint;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.functions.KeyedProcessFunction;
import org.apache.flink.util.Collector;

import java.util.List;


@Slf4j
@Getter
@Setter
public abstract class AbstractDecisionMaker<T> extends KeyedProcessFunction<String, Tuple2<SignalGroupKey, List<IndicatorSignal>>, T> {

    private transient SnowFlakeFactory snowFlakeFactory;

    private final String name;

    /**
     * 存放历史信号
     */
    private MapState<String, Tuple2<SignalGroupKey, List<IndicatorSignal>>> historySignalMapState;

    protected AbstractDecisionMaker(String name) {
        this.name = name;
    }

    @Override
    public void open(Configuration parameters) throws Exception {
        historySignalMapState = getRuntimeContext().getMapState(new MapStateDescriptor<>("historySignalMapState", BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.of(new TypeHint<>() {
        })));
        SnowFlowConfig snowFlow = TradeSignalConfig.TRADE_SIGNAL_CONFIG.getRun_type().getSnow_flow();
        snowFlakeFactory = new SnowFlakeFactory(snowFlow);
    }

    @Override
    public void processElement(Tuple2<SignalGroupKey, List<IndicatorSignal>> kLineListTuple2, KeyedProcessFunction<String, Tuple2<SignalGroupKey, List<IndicatorSignal>>, T>.Context context, Collector<T> collector) throws Exception {

        SignalGroupKey key = kLineListTuple2.getField(0);
        String symbol = key.getSymbol();
        List<IndicatorSignal> signals = kLineListTuple2.getField(1);

        if (signals.isEmpty()) {
            log.debug("[{}] - symbol[{}]时间窗口内没有信号", symbol, name);
        } else {
            log.debug("[{}] - symbol[{}]当前时间窗口，产生[{}]个信号", name, signals, signals.size());
            T out = decisionAndBuilderOrder(symbol, signals, null);

            //更新历史信号
            historySignalMapState.put(key.getStreamKey(), kLineListTuple2);

            if (out != null) {
                log.info("[{}] - symbol[{}]窗口内信号满足决策下单条件，下单[{}}", name, symbol, out);
                collector.collect(out);
            }
        }
    }

    protected abstract T decisionAndBuilderOrder(String symbol, List<IndicatorSignal> windowSignal, IndicatorMap indicatorMap);


    /**
     * 下一个信号id，采用雪花算法
     *
     * @return id
     */
    protected String nextSignalId() {
        return snowFlakeFactory.nextId(BRStyle.TRADE_SIGNAL);
    }

    /**
     * 取历史信号
     *
     * @param kLineStreamKey kLine的key，symbol + interval
     * @return 历史信号
     * @throws Exception 异常
     */
    private Tuple2<SignalGroupKey, List<IndicatorSignal>> getHistorySignal(String kLineStreamKey) throws Exception {
        return historySignalMapState.get(kLineStreamKey);
    }
}


shinano:
  quantity:
    # 信号生成服务配置
    trade_signal_maker:
      # name
      name: test
      # symbol
      symbol: btcusdt

      run_type:
        configs:
          - env: NORMAL  # 运行环境
            trade_type:  # 交易类型
              - SPOT
        snow_flow: # 雪花算法配置
          datacenter_id: 0 # 数据中心id
          machine_id: 1 # 机器id

      # 历史k线加载批大小
      historyKLineBatchSize: 200
      # 批加载的网络并发度
      batchLoadConcurrent: 10
      # 实时数据配置
      realtime:
        # kafka配置
        kafka:
          input:
            bootstrapServer: 192.168.1.2:9092
            groupId: trade_signal_app_test
            transaction_timeout_ms: 900000

          output:
            bootstrapServer: 192.168.1.2:9092
            groupId: trade_signal_app_test
            transaction_timeout_ms: 900000

        # flink配置
        flink:
          jobManagerHost: 192.168.1.2
          jobManagerPort: 8081

