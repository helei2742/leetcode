package com.helei.dto.account;

import com.helei.constants.CEXType;
import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeType;
import com.helei.dto.ASKey;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.util.List;
import java.util.concurrent.atomic.AtomicBoolean;


@Data
@AllArgsConstructor
@NoArgsConstructor
@Builder
public class UserAccountStaticInfo {

    /**
     * 账户id
     */
    private long id;

    /**
     * 用户id
     */
    private long userId;

    /**
     * 验证key
     */
    private ASKey asKey;

    /**
     * 是否可用
     */
    private volatile boolean usable = false;

    /**
     * 运行环境，测试网还是主网
     */
    private RunEnv runEnv;

    /**
     * 交易类型
     */
    private TradeType tradeType;

    /**
     * 账户交易所类型
     */
    private CEXType cexType = CEXType.BINANCE;

    /**
     * 交易币种
     */
    private final String quote = "USDT";

    /**
     * 订阅的交易对
     */
    private List<String> subscribeSymbol;


    /**
     * 账户仓位设置
     */
    private AccountPositionConfig accountPositionConfig;
}


package com.helei.dto.trade;

import com.helei.constants.CEXType;
import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeSide;
import com.helei.constants.trade.TradeType;
import lombok.*;

import java.math.BigDecimal;


/**
 * 原始订单数据
 */
@Data
@AllArgsConstructor
@EqualsAndHashCode
@Builder
public class TradeSignal {

    /**
     * 信号id
     */
    private String id;

    /**
     * 运行环境
     */
    private RunEnv runEnv;

    /**
     * 交易类型
     */
    private TradeType tradeType;

    /**
     * 交易所类型
     */
    private CEXType cexType;

    /**
     * 交易对
     */
    private String symbol;

    /**
     * 交易方向
     */
    private TradeSide tradeSide;

    /**
     * 目标价格
     */
    private BigDecimal targetPrice;

    /**
     * 进场价格
     */
    private BigDecimal enterPrice;

    /**
     * 止损价格
     */
    private BigDecimal stopPrice;

    /**
     * 信号创建时间戳
     */
    private long createTimestamp;

    /**
     * 信号创建的k线open时间
     */
    private long createKLineOpenTimestamp;
}

package com.helei.util;

import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeType;


public class RedisKeyUtil {

    private static final String USER_Info_PREFIX = "user";

    /**
     * Redis中存放相应环境数据的前缀
     *
     * @param runEnv    runEnv
     * @param tradeType tradeType
     * @return prefix
     */
    public static String getEnvKeyPrefix(RunEnv runEnv, TradeType tradeType) {
        return (runEnv.name() + ":" + tradeType.name() + ":").toLowerCase();
    }

    /**
     * Redis中存放用户数据的前缀
     *
     * @param runEnv    runEnv
     * @param tradeType tradeType
     * @return prefix
     */
    public static String getUserEnvKeyPrefix(RunEnv runEnv, TradeType tradeType) {
        return getEnvKeyPrefix(runEnv, tradeType) + USER_Info_PREFIX + ":";
    }

    /**
     * redis中存放用户具体数据的前缀
     *
     * @param runEnv    runEnv
     * @param tradeType tradeType
     * @return prefix
     */
    public static String getUserInfoKeyPrefix(RunEnv runEnv, TradeType tradeType) {
        return getUserEnvKeyPrefix(runEnv, tradeType) + "id:";
    }

    /**
     * redis中存放用户具体数据的匹配模式
     *
     * @param runEnv    runEnv
     * @param tradeType tradeType
     * @return pattern
     */
    public static String getUserBaseInfoPattern(RunEnv runEnv, TradeType tradeType) {
        return getUserInfoKeyPrefix(runEnv, tradeType) + "*base*";
    }

    /**
     * 获取用户账户实时数据的key
     *
     * @param runEnv    runEnv
     * @param tradeType tradeType
     * @return String
     */
    public static String getUserAccountEnvRTDataHashKey(RunEnv runEnv, TradeType tradeType, long userId) {
        return getUserInfoKeyPrefix(runEnv, tradeType) + userId + ":realtime_account_data";
    }

    /**
     * 获取用户账户历史数据的key
     *
     * @param runEnv    runEnv
     * @param tradeType tradeType
     * @return String
     */
    public static String getUserAccountEnvStaticDataHashKey(RunEnv runEnv, TradeType tradeType, long userId) {
        return getUserInfoKeyPrefix(runEnv, tradeType) + userId + ":static_account_data";
    }


    /**
     * 用户基础数据的key
     *
     * @param env       env
     * @param tradeType tradeType
     * @param userId    userId
     * @return key
     */
    public static String getUserBaseInfoKey(RunEnv env, TradeType tradeType, long userId) {
        return getUserInfoKeyPrefix(env, tradeType) + userId + ":base";
    }

}



package com.helei.realtimedatacenter.service.impl.market;

import cn.hutool.core.collection.ListUtil;
import cn.hutool.core.lang.Pair;
import com.alibaba.fastjson.JSONObject;
import com.helei.binanceapi.base.SubscribeResultInvocationHandler;
import com.helei.binanceapi.config.BinanceApiConfig;
import com.helei.constants.CEXType;
import com.helei.constants.RunEnv;
import com.helei.constants.WebSocketStreamParamKey;
import com.helei.constants.trade.KLineInterval;
import com.helei.constants.trade.TradeType;
import com.helei.dto.base.KeyValue;
import com.helei.realtimedatacenter.config.RealtimeConfig;
import com.helei.realtimedatacenter.dto.SymbolKLineInfo;
import com.helei.realtimedatacenter.service.MarketRealtimeDataService;
import com.helei.realtimedatacenter.service.impl.KafkaProducerService;
import com.helei.util.KafkaUtil;
import lombok.extern.slf4j.Slf4j;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.*;


/**
 * 市场实时数据服务的抽象类
 * <P>能够将市场数据推送至kafka，会根据配置文件中的run_type来加载需要使用的环境。只需关注实现registryKLineDataLoader(*)方法</P>
 */
@Slf4j
public abstract class AbstractKafkaMarketRTDataService implements MarketRealtimeDataService {

    protected final ExecutorService taskExecutor;

    public final KafkaProducerService kafkaProducerService;

    protected final RealtimeConfig realtimeConfig;

    protected final BinanceApiConfig binanceApiConfig;

    protected final CEXType cexType;

    public AbstractKafkaMarketRTDataService(ExecutorService taskExecutor, KafkaProducerService kafkaProducerService, CEXType cexType) {
        this.taskExecutor = taskExecutor;
        this.kafkaProducerService = kafkaProducerService;
        this.cexType = cexType;
        this.realtimeConfig = RealtimeConfig.INSTANCE;
        this.binanceApiConfig = BinanceApiConfig.INSTANCE;
    }

    @Override
    public Integer startSyncRealTimeKLine() {
        int all = 0;
        List<CompletableFuture<Integer>> futures = new ArrayList<>();

        for (KeyValue<RunEnv, TradeType> keyValue : realtimeConfig.getRun_type().getRunTypeList()) {
            futures.add(CompletableFuture.supplyAsync(() -> startSyncRealTimeKLine(keyValue.getKey(), keyValue.getValue()), taskExecutor));
        }

        for (CompletableFuture<Integer> future : futures) {
            try {
                all += future.get();
            } catch (InterruptedException | ExecutionException e) {
                throw new RuntimeException(e);
            }
        }
        return all;
    }

    @Override
    public Integer startSyncRealTimeKLine(RunEnv runEnv, TradeType tradeType) {
        log.info("开始同步env[{}]-tradeType[{}]的实时k线", runEnv, tradeType);

        RealtimeConfig.RealtimeKLineDataConfig realtimeKLineDataConfig = realtimeConfig.getEnvKLineDataConfig(runEnv, tradeType);

        //Step 1: 解析k线
        List<SymbolKLineInfo> realtimeKLineList = realtimeKLineDataConfig.getRealtimeKLineList();

        if (realtimeKLineList == null || realtimeKLineList.isEmpty()) {
            log.warn("runEnv[{}]-tradeType[{}] 没有设置要实时获取的k线", runEnv, tradeType);
            return 0;
        }

        List<Pair<String, KLineInterval>> intervals = new ArrayList<>();

        for (SymbolKLineInfo symbolKLineInfo : realtimeKLineList) {
            symbolKLineInfo.getIntervals().forEach(interval -> {
                intervals.add(new Pair<>(symbolKLineInfo.getSymbol(), interval));
            });
        }

        //Step 2: 创建topic
        log.info("开始检查并创建所需topic");
        createTopic(intervals, runEnv, tradeType);
        log.info("topic创建完毕");


        //Step 3: 分片执行
        List<List<Pair<String, KLineInterval>>> partition = ListUtil.partition(intervals, realtimeKLineDataConfig.getClient_listen_kline_max_count());


        try {
            List<CompletableFuture<Void>> futures = new ArrayList<>();
            for (List<Pair<String, KLineInterval>> list : partition) {

                //Step 4: 创建task执行获取
                CompletableFuture<Void> future = registryKLineDataLoader(
                        runEnv,
                        tradeType,
                        list,
                        (s, p, k) -> klineDataSyncToKafka(s, (KLineInterval) p.get(WebSocketStreamParamKey.KLINE_INTERVAL), k, runEnv, tradeType),
                        taskExecutor);

                futures.add(future);
            }

            CompletableFuture
                    .allOf(futures.toArray(new CompletableFuture[0]))
                    .get();

            log.info("所有k线开始实时同步");
        } catch (Exception e) {
            throw new RuntimeException(e);
        }

        return realtimeKLineList.size();
    }


    /**
     * 注册k线数据加载器
     *
     * @param runEnv               运行环境
     * @param tradeType            交易类型
     * @param listenKLines         k线
     * @param whenReceiveKLineData 回调，需要在whenReceiveKLineData.invoke()时传入symbol、interval、json格式的k线数据
     * @param executorService      执行的线程池
     * @return CompletableFuture
     */
    protected abstract CompletableFuture<Void> registryKLineDataLoader(
            RunEnv runEnv,
            TradeType tradeType,
            List<Pair<String, KLineInterval>> listenKLines,
            SubscribeResultInvocationHandler whenReceiveKLineData,
            ExecutorService executorService
    ) throws ExecutionException, InterruptedException;

    /**
     * 把k线发到kafka
     *
     * @param symbol symbol
     * @param data   data
     */
    public void klineDataSyncToKafka(String symbol, KLineInterval kLineInterval, JSONObject data, RunEnv runEnv, TradeType tradeType) {
        String topic = KafkaUtil.resolveKafkaTopic(cexType, KafkaUtil.getKLineStreamName(symbol, kLineInterval), runEnv, tradeType);

        log.info("收到k线信息 - {}, - {} - {} - {} send to topic[{}]", symbol, data, runEnv, tradeType, topic);
        try {
            kafkaProducerService.sendMessage(
                    topic,
                    data.toJSONString()
            ).get();
        } catch (InterruptedException | ExecutionException e) {
            log.error("保持k线信息到kafka出错，symbol[{}]", symbol, e);
        }
    }

    /**
     * 创建topic
     *
     * @param kLines    k线list
     * @param runEnv    运行环境
     * @param tradeType 交易类型
     */
    private void createTopic(List<Pair<String, KLineInterval>> kLines, RunEnv runEnv, TradeType tradeType) {
        for (Pair<String, KLineInterval> kLine : kLines) {
            String topic = KafkaUtil.resolveKafkaTopic(CEXType.BINANCE, KafkaUtil.getKLineStreamName(kLine.getKey(), kLine.getValue()), runEnv, tradeType);

            kafkaProducerService.checkAndCreateTopic(
                    topic,
                    realtimeConfig.getKafka().getKafka_num_partitions(),
                    realtimeConfig.getKafka().getKafka_replication_factor()
            );
        }
    }
}

package com.helei.realtimedatacenter.service.impl.market;

import cn.hutool.core.lang.Pair;
import com.helei.binanceapi.BinanceWSMarketStreamClient;
import com.helei.binanceapi.base.AbstractBinanceWSApiClient;
import com.helei.binanceapi.base.SubscribeResultInvocationHandler;
import com.helei.binanceapi.constants.BinanceWSClientType;
import com.helei.cexapi.manager.BinanceBaseClientManager;
import com.helei.constants.CEXType;
import com.helei.constants.trade.KLineInterval;
import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeType;
import com.helei.realtimedatacenter.manager.ExecutorServiceManager;
import com.helei.realtimedatacenter.realtime.impl.BinanceKLineRTDataSyncTask;
import com.helei.realtimedatacenter.service.impl.KafkaProducerService;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;


/**
 * 币安市场数据服务
 */
@Slf4j
@Service
public class BinanceMarketRTDataService extends AbstractKafkaMarketRTDataService {

    @Autowired
    private BinanceBaseClientManager binanceBaseClientManager;


    @Autowired
    public BinanceMarketRTDataService(ExecutorServiceManager executorServiceManager, KafkaProducerService kafkaProducerService) {
        super(executorServiceManager.getKlineTaskExecutor(), kafkaProducerService, CEXType.BINANCE);
    }

    @Override
    protected CompletableFuture<Void> registryKLineDataLoader(
            RunEnv runEnv,
            TradeType tradeType,
            List<Pair<String, KLineInterval>> listenKLines,
            SubscribeResultInvocationHandler whenReceiveKLineData,
            ExecutorService executorService
    ) throws ExecutionException, InterruptedException {
        AbstractBinanceWSApiClient client = binanceBaseClientManager.getEnvTypedApiClient(runEnv, tradeType, BinanceWSClientType.MARKET_STREAM).get();
        BinanceWSMarketStreamClient marketStreamClient = (BinanceWSMarketStreamClient) client;

        return new BinanceKLineRTDataSyncTask(
                marketStreamClient,
                listenKLines
        ).startSync(whenReceiveKLineData, taskExecutor);
    }
}


package com.helei.realtimedatacenter.service.impl.market;

import cn.hutool.core.lang.Pair;
import com.alibaba.fastjson.JSONObject;
import com.helei.binanceapi.base.SubscribeResultInvocationHandler;
import com.helei.constants.CEXType;
import com.helei.constants.RunEnv;
import com.helei.constants.WebSocketStreamParamKey;
import com.helei.constants.trade.KLineInterval;
import com.helei.constants.trade.TradeType;
import com.helei.realtimedatacenter.manager.ExecutorServiceManager;
import com.helei.realtimedatacenter.service.impl.KafkaProducerService;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.time.ZoneOffset;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.concurrent.*;


/**
 * 随机市场数据服务
 */
@Service
public class RandomMarketRTDataService extends AbstractKafkaMarketRTDataService {

    private final Random random = new Random();

    private final Double maxPrice = 99999.09;

    private final Double minPrice = 111.229;

    private final ConcurrentHashMap<String, ConcurrentHashMap<KLineInterval, Long>> startTimeStampMap;

    private final ConcurrentHashMap<String, ConcurrentHashMap<KLineInterval, Long>> realTimerMap;

    private final LocalDateTime startTimeStamp = LocalDateTime.of(2022, 1, 1, 1, 1);

    private long epochMilli;


    public RandomMarketRTDataService(
            ExecutorServiceManager executorServiceManager, KafkaProducerService kafkaProducerService
    ) {
        super(executorServiceManager.getKlineTaskExecutor(), kafkaProducerService, CEXType.BINANCE);

        epochMilli = startTimeStamp.toInstant(ZoneOffset.UTC).toEpochMilli();

        if (epochMilli > System.currentTimeMillis()) {
            epochMilli = System.currentTimeMillis();
        }
        this.startTimeStampMap = new ConcurrentHashMap<>();
        this.realTimerMap = new ConcurrentHashMap<>();
    }

    @Override
    protected CompletableFuture<Void> registryKLineDataLoader(
            RunEnv runEnv,
            TradeType tradeType,
            List<Pair<String, KLineInterval>> listenKLines,
            SubscribeResultInvocationHandler whenReceiveKLineData,
            ExecutorService executorService
    ) {
        return CompletableFuture.runAsync(() -> {
            String key = getKey(runEnv, tradeType);
            for (Pair<String, KLineInterval> listenKLine : listenKLines) {
                String symbol = listenKLine.getKey();
                KLineInterval interval = listenKLine.getValue();

                startTimeStampMap.putIfAbsent(key, new ConcurrentHashMap<>());
                realTimerMap.putIfAbsent(key, new ConcurrentHashMap<>());

                startTimeStampMap.get(key).putIfAbsent(interval, epochMilli);
                realTimerMap.get(key).putIfAbsent(interval, epochMilli);

                executorService.execute(() -> {
                    Map<String, Object> map = new HashMap<>();
                    map.put(WebSocketStreamParamKey.KLINE_INTERVAL, interval);
                    while (true) {
                        try {
                            JSONObject kLine = loadKLine(runEnv, tradeType, symbol, interval);
                            whenReceiveKLineData.invoke(symbol, map, kLine);
                            TimeUnit.SECONDS.sleep(10);
                        } catch (InterruptedException e) {
                            throw new RuntimeException(e);
                        }
                    }
                });
            }

        }, executorService);
    }


    protected JSONObject loadKLine(
            RunEnv runEnv,
            TradeType tradeType,
            String symbol,
            KLineInterval kLineInterval
    ) {

        double nextLow = minPrice + (maxPrice - minPrice) * random.nextDouble();
        double nextHigh = nextLow + (maxPrice - nextLow) * random.nextDouble();
        double nextOpen = nextLow + (nextHigh - nextLow) * random.nextDouble();
        double nextClose = nextLow + (nextHigh - nextLow) * random.nextDouble();

        double volume = 10 + (Double.MAX_VALUE / 2 - 10) * random.nextDouble();
        long plus = kLineInterval.getSecond() * 1000;
        String key = getKey(runEnv, tradeType);
        long openTime = startTimeStampMap.get(key).get(kLineInterval);

        realTimerMap.get(key).computeIfPresent(kLineInterval, (k, v) -> v + 200);
        long curTime = realTimerMap.get(key).get(kLineInterval);

        boolean isRealTime = curTime > System.currentTimeMillis() - kLineInterval.getSecond() * 1000;
        if (isRealTime) {
            if (curTime >= openTime + plus) {
                openTime += plus;
                startTimeStampMap.get(key).put(kLineInterval, openTime);
            }
        } else {
            openTime += plus;
            startTimeStampMap.get(key).put(kLineInterval, openTime);
        }

        JSONObject jb = new JSONObject();
        jb.put("t", openTime);
        jb.put("T", openTime + plus - 1000);
        jb.put("s", symbol);
        jb.put("h", nextHigh);
        jb.put("l", nextLow);
        jb.put("o", nextOpen);
        jb.put("c", nextClose);
        jb.put("v", volume);
        jb.put("x", !isRealTime);
        jb.put("i", kLineInterval.name());

        return jb;
    }

    private String getKey(RunEnv runEnv, TradeType tradeType) {
        return runEnv.name() + " - " + tradeType.name();
    }
}






package com.helei.realtimedatacenter.service.impl;

import com.alibaba.fastjson.JSONObject;
import com.alibaba.fastjson.serializer.SimplePropertyPreFilter;
import com.helei.binanceapi.BinanceWSReqRespApiClient;
import com.helei.binanceapi.constants.BinanceWSClientType;
import com.helei.cexapi.manager.BinanceBaseClientManager;
import com.helei.constants.RunEnv;
import com.helei.constants.order.OrderType;
import com.helei.constants.trade.TradeType;
import com.helei.dto.ASKey;
import com.helei.dto.account.*;
import com.helei.dto.base.KeyValue;
import com.helei.realtimedatacenter.config.RealtimeConfig;
import com.helei.realtimedatacenter.manager.ExecutorServiceManager;
import com.helei.realtimedatacenter.service.UserService;
import com.helei.realtimedatacenter.supporter.BatchWriteSupporter;
import com.helei.util.RedisKeyUtil;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;


@Slf4j
@Service
public class UserServiceImpl implements UserService {

    private final RealtimeConfig realtimeConfig = RealtimeConfig.INSTANCE;

    private static final SimplePropertyPreFilter propertyPreFilter;

    @Autowired
    private BatchWriteSupporter batchWriteSupporter;

    @Autowired
    private BinanceBaseClientManager binanceBaseClientManager;

    private final ExecutorService executor;

    static {
        propertyPreFilter = new SimplePropertyPreFilter();
        propertyPreFilter.getExcludes().add("accountInfos");
    }

    @Autowired
    public UserServiceImpl(ExecutorServiceManager executorServiceManager) {
        executor = executorServiceManager.getSyncTaskExecutor();
    }


    @Override
    public List<UserInfo> queryAll() {

        for (KeyValue<RunEnv, TradeType> keyValue : realtimeConfig.getRun_type().getRunTypeList()) {
            //TODO 查数据库, 只取这些环境里的

        }
        List<UserInfo> list = new ArrayList<>();

        UserInfo u_contract_test_net_account = UserInfo.builder()
                .id(1)
                .username("合约测试网账号")
                .password("123456")

                .accountInfos(List.of(
                        UserAccountInfo
                                .builder()
                                .id(1)
                                .userId(1)
                                .userAccountStaticInfo(
                                        UserAccountStaticInfo
                                                .builder()
                                                .id(1)
                                                .userId(1)
                                                .accountPositionConfig(AccountPositionConfig
                                                        .builder()
                                                        .riskPercent(0.5)
                                                        .orderType(OrderType.LIMIT)
                                                        .leverage(10)
                                                        .build()
                                                )
                                                .usable(true)
                                                .asKey(new ASKey("b252246c6c6e81b64b8ff52caf6b8f37471187b1b9086399e27f6911242cbc66", "a4ed1b1addad2a49d13e08644f0cc8fc02a5c14c3511d374eac4e37763cadf5f"))
                                                .subscribeSymbol(List.of("btcusdt", "ethusdt", "solusdt"))
                                                .runEnv(RunEnv.TEST_NET)
                                                .tradeType(TradeType.CONTRACT)
                                                .build()
                                )
                                .build()
                ))
                .build();
        UserInfo spot_test_net_account = UserInfo.builder()
                .id(2)
                .username("现货测试网账号")
                .password("123456")
                .accountInfos(List.of(
                        UserAccountInfo
                                .builder()
                                .id(2)
                                .userId(2)
                                .userAccountStaticInfo(
                                        UserAccountStaticInfo
                                                .builder()
                                                .id(2)
                                                .userId(2)
                                                .accountPositionConfig(AccountPositionConfig
                                                        .builder()
                                                        .riskPercent(0.5)
                                                        .orderType(OrderType.LIMIT)
                                                        .leverage(10)
                                                        .build()
                                                )
                                                .usable(true)
                                                .subscribeSymbol(List.of("btcusdt", "ethusdt", "solusdt"))
                                                .asKey(new ASKey("1JIhkPyK07xadG9x8hIwqitN95MgpypPzA4b6TLraTonRnJ8BBJQlaO2iL9tPH0Y", "t84TYFR1zieMGncbw3kYq4zAPLxIJHJeMdD8V0FMKxij9fApojV6bhbDpyyjNDWt"))
                                                .runEnv(RunEnv.TEST_NET)
                                                .tradeType(TradeType.SPOT)
                                                .build()
                                )
                                .build()
                ))
                .build();

        UserInfo binance_account = UserInfo.builder()
                .id(3)
                .username("正式网账号")
                .password("123456")
                .accountInfos(List.of(
                        UserAccountInfo
                                .builder()
                                .id(3)
                                .userId(3)
                                .userAccountStaticInfo(
                                        UserAccountStaticInfo
                                                .builder()
                                                .id(3)
                                                .userId(3)
                                                .accountPositionConfig(AccountPositionConfig
                                                        .builder()
                                                        .riskPercent(0.5)
                                                        .orderType(OrderType.LIMIT)
                                                        .leverage(10)
                                                        .build()
                                                )
                                                .usable(true)
                                                .subscribeSymbol(List.of("btcusdt", "ethusdt", "solusdt"))
                                                .asKey(new ASKey("TUFsFL4YrBsR4fnBqgewxiGfL3Su5L9plcjZuyRO3cq6M1yuwV3eiNX1LcMamYxz", "YsLzVacYo8eOGlZZ7RjznyWVjPHltIXzZJz2BrggCmCUDcW75FyFEv0uKyLBVAuU"))
                                                .runEnv(RunEnv.NORMAL)
                                                .tradeType(TradeType.SPOT)
                                                .build()
                                )
                                .build()
                ))
                .build();

        list.add(u_contract_test_net_account);
        list.add(spot_test_net_account);
        list.add(binance_account);
        return list;
    }

    @Override
    public List<UserInfo> queryEnvUser(RunEnv runEnv, TradeType tradeType) {
        //TODO 测试阶段，写死的

        return List.of();
    }

    @Override
    public void updateUserAccountRTInfo(RunEnv runEnv, TradeType tradeType, UserAccountRealTimeInfo realTimeInfo) {

        long accountId = realTimeInfo.getId();
        long userId = realTimeInfo.getUserId();

        String key = RedisKeyUtil.getUserAccountEnvRTDataHashKey(runEnv, tradeType, userId);
        String hashKey = String.valueOf(accountId);

        //只发实时的部分数据
        String value = JSONObject.toJSONString(realTimeInfo);

        log.debug("更新账户实时信息，key[{}], value[{}]", key, value);

        batchWriteSupporter.writeToRedisHash(key, hashKey, value);
    }

    @Override
    public void updateUserAccountStaticInfo(RunEnv runEnv, TradeType tradeType, UserAccountStaticInfo staticInfo) {
        long accountId = staticInfo.getId();
        long userId = staticInfo.getUserId();

        String key = RedisKeyUtil.getUserAccountEnvStaticDataHashKey(runEnv, tradeType, userId);
        String hashKey = String.valueOf(accountId);

        //只发实时的部分数据
        String value = JSONObject.toJSONString(staticInfo);

        log.debug("更新账户静态信息，key[{}], value[{}]", key, value);

        batchWriteSupporter.writeToRedisHash(key, hashKey, value);
    }

    /**
     * 更新用户的baseInfo 到redis
     *
     * @param env       运行环境
     * @param tradeType 交易类型
     * @param userInfo  用户信息
     */
    @Override
    public void updateUserBaseInfoToRedis(RunEnv env, TradeType tradeType, UserInfo userInfo) {
        String key = RedisKeyUtil.getUserBaseInfoKey(env, tradeType, userInfo.getId());
        batchWriteSupporter.writeToRedis(key, JSONObject.toJSONString(userInfo, propertyPreFilter));
    }


    /**
     * 更新UserInfo到Redis，包括User名下的账户信息
     *
     * @param env       运行环境
     * @param tradeType 交易类型
     */
    public void updateEnvAllUserInfoToRedis(RunEnv env, TradeType tradeType) {
        List<UserInfo> userInfos = queryAll();
        try {
            BinanceWSReqRespApiClient requestClient = (BinanceWSReqRespApiClient) binanceBaseClientManager.getEnvTypedApiClient(env, tradeType, BinanceWSClientType.REQUEST_RESPONSE).get();

            //Step 1 遍历用户
            for (UserInfo userInfo : userInfos) {
                updateUserInfoTpRedis(env, tradeType, userInfo, requestClient);
            }
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException(e);
        }
    }

    /**
     * 更新用户信息到redis
     *
     * @param env           env
     * @param tradeType     tradeType
     * @param userInfo      userInfo
     * @param requestClient requestClient
     * @throws InterruptedException InterruptedException
     * @throws ExecutionException   ExecutionException
     */
    private void updateUserInfoTpRedis(RunEnv env, TradeType tradeType, UserInfo userInfo, BinanceWSReqRespApiClient requestClient) throws InterruptedException, ExecutionException {
        Map<CompletableFuture<JSONObject>, UserAccountStaticInfo> futuresMap = new HashMap<>();
        Set<Long> accountIds = new HashSet<>();

        //Step 2 遍历用户下的账户静态数据，获取详细信息（实时数据）
        for (UserAccountInfo accountInfo : userInfo.getAccountInfos()) {
            UserAccountStaticInfo staticInfo = accountInfo.getUserAccountStaticInfo();

            if (!staticInfo.getRunEnv().equals(env) || !staticInfo.getTradeType().equals(tradeType)) {
                log.warn("userId[{}]-accountId[{}] 不能在当前环境[{}]-[{}]下运行", accountInfo.getUserId(), accountInfo.getId(), env, tradeType);
                continue;
            }

            CompletableFuture<JSONObject> accountStatusFuture = requestClient
                    .getAccountApi()
                    .accountStatus(staticInfo.getAsKey(), true)
                    .thenApplyAsync(jb -> {
                        // 记录成功同步信息的id
                        accountIds.add(accountInfo.getId());
                        return jb;
                    }, executor);
            futuresMap.put(accountStatusFuture, staticInfo);
        }

        //Step 3 解析详细信息（实时数据），放入UserAccountInfo，并写入redis
        CompletableFuture
                .allOf(futuresMap.keySet().toArray(new CompletableFuture[0]))
                .whenCompleteAsync((unused, throwable) -> {
                    if (throwable != null) {
                        log.error("userId[{}}获取最新账户信息发生错误", userInfo.getId(), throwable);
                    }
                    futuresMap.forEach((future, staticInfo) -> {
                        long userId = staticInfo.getUserId();
                        long accountId = staticInfo.getId();

                        try {
                            JSONObject result = future.get();

                            log.info("获取到userId[{}]-accountId[{}]最新的账户信息 [{}]", userId, accountId, result);

                            //解析结构, 创建账户实时信息
                            UserAccountRealTimeInfo realTimeInfo = UserAccountRealTimeInfo.generateAccountStatusFromJson(result);
                            realTimeInfo.setUserId(userId);
                            realTimeInfo.setId(accountId);

                            //写redis
                            updateUserAccountStaticInfo(staticInfo.getRunEnv(), staticInfo.getTradeType(), staticInfo);
                            updateUserAccountRTInfo(staticInfo.getRunEnv(), staticInfo.getTradeType(), realTimeInfo);
                        } catch (InterruptedException | ExecutionException e) {
                            accountIds.remove(staticInfo.getId());
                            throw new RuntimeException(String.format("userId[%s]-accountId[%s]获取最新账户信息发生错误", userId, accountId), e);
                        }
                    });
                }, executor)
                .get();

        userInfo.setAccountIds(accountIds);
        log.info("userId[{}] 所有runEnv[{}]-tradeType[{}]的账户信息初始化完毕", userInfo.getId(), env, tradeType);


        //Step 4 User 数据写入Redis
        updateUserBaseInfoToRedis(env, tradeType, userInfo);
    }


    /**
     * 更新所有的用户信息到redis
     */
    @Override
    public void updateAllUserInfo() {
        for (KeyValue<RunEnv, TradeType> keyValue : realtimeConfig.getRun_type().getRunTypeList()) {
            updateEnvAllUserInfoToRedis(keyValue.getKey(), keyValue.getValue());
        }
    }
}



package com.helei.realtimedatacenter.service;

import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeType;
import com.helei.dto.account.UserAccountRealTimeInfo;
import com.helei.dto.account.UserAccountStaticInfo;
import com.helei.dto.account.UserInfo;

import java.util.List;

public interface UserService {

    /**
     * 查询所有user
     *
     * @return UserInfo list
     */
    List<UserInfo> queryAll();

    /**
     * 查询指定环境的用户
     *
     * @param runEnv    运行环境
     * @param tradeType 交易类型
     * @return user list
     */
    List<UserInfo> queryEnvUser(RunEnv runEnv, TradeType tradeType);

    /**
     * 更新用户账户实时信息
     *
     * @param tradeType    tradeType
     * @param runEnv       runEnv
     * @param realTimeInfo realTimeInfo
     */
    void updateUserAccountRTInfo(RunEnv runEnv, TradeType tradeType, UserAccountRealTimeInfo realTimeInfo);


    /**
     * 更新用户账户历史信息
     *
     * @param tradeType    tradeType
     * @param runEnv       runEnv
     * @param staticInfo staticInfo
     */
    void updateUserAccountStaticInfo(RunEnv runEnv, TradeType tradeType, UserAccountStaticInfo staticInfo);

    /**
     * 更新用户的baseInfo 到redis
     *
     * @param env       运行环境
     * @param tradeType 交易类型
     * @param userInfo  用户信息
     */
    void updateUserBaseInfoToRedis(RunEnv env, TradeType tradeType, UserInfo userInfo);

    /**
     * 更新所有用户信息
     */
    void updateAllUserInfo();
}





package com.helei.realtimedatacenter.supporter;


import com.helei.dto.base.KeyValue;
import com.helei.realtimedatacenter.manager.ExecutorServiceManager;
import lombok.extern.slf4j.Slf4j;
import org.redisson.api.RBucket;
import org.redisson.api.RedissonClient;
import org.springframework.beans.factory.InitializingBean;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.ApplicationListener;
import org.springframework.context.event.ContextClosedEvent;
import org.springframework.stereotype.Component;

import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;

@Slf4j
@Component
public class BatchWriteSupporter implements InitializingBean, ApplicationListener<ContextClosedEvent> {


    /**
     * redis 批量写入的阀值
     */
    private static final int REDIS_BATCH_SIZE = 5;

    /**
     * redis 批量写入的间隔
     */
    private static final int REDIS_BATCH_INTERVAL = 1000;


    /**
     * 是否在运行
     */
    private volatile boolean isRunning = true;


    /**
     * 缓存redis key - < UpdateCount,value>
     */
    private final ConcurrentHashMap<String, KeyValue<Integer, String>> redisKVMap = new ConcurrentHashMap<>();

    @Autowired
    private RedissonClient redissonClient;

    @Autowired
    private ExecutorServiceManager executorServiceManager;


    /**
     * 写入redis
     *
     * @param key   key
     * @param value value
     */
    public void writeToRedis(String key, String value) {
        AtomicBoolean isWrite = new AtomicBoolean(false);

        redisKVMap.compute(key, (k, v) -> {
            if (v == null) {
                v = new KeyValue<>(0, "");
            }
            v.setKey(v.getKey() + 1);
            v.setValue(value);

            if (v.getKey() >= REDIS_BATCH_SIZE) isWrite.set(true);
            return v;
        });

        if (isWrite.get()) {
            batchWriteRedis(key);
        }
    }


    /**
     * 写入redis
     *
     * @param key redisKVMap 的key，也是写入redis的key
     */
    private void batchWriteRedis(String key) {
        KeyValue<Integer, String> remove = redisKVMap.remove(key);
        if (remove != null) {
            RBucket<Object> bucket = redissonClient.getBucket(key);
            bucket.set(remove.getValue());  // 写入 Redis
        }
    }

    private void batchWriteTask() {
        try {
            while (isRunning) {
                //1.处理redis的
                for (Map.Entry<String, KeyValue<Integer, String>> entry : redisKVMap.entrySet()) {
                    batchWriteRedis(entry.getKey());
                }

                //睡眠一会
                TimeUnit.MILLISECONDS.sleep(REDIS_BATCH_INTERVAL);
            }
        } catch (Exception e) {
            log.error("运行批量写入任务失败", e);
        }
    }


    /**
     * 关闭
     */
    public void shutdown() {
        isRunning = false;
    }


    @Override
    public void afterPropertiesSet() throws Exception {
        executorServiceManager.getSyncTaskExecutor().execute(this::batchWriteTask);
    }


    /**
     * 写入RedisHash结构
     * @param key key
     * @param hashKey hash 的key
     * @param value 值
     */
    public void writeToRedisHash(String key, String hashKey, String value) {
        redissonClient.getMap(key).putAsync(hashKey, value);
    }

    @Override
    public void onApplicationEvent(ContextClosedEvent event) {
        try {
            for (Map.Entry<String, KeyValue<Integer, String>> entry : redisKVMap.entrySet()) {
                batchWriteRedis(entry.getKey());
            }
        }catch (Exception e) {
            throw new RuntimeException(e);
        }
    }
}




package com.helei.realtimedatacenter;

import com.helei.realtimedatacenter.service.AccountEventStreamService;
import com.helei.realtimedatacenter.service.MarketRealtimeDataService;
import com.helei.realtimedatacenter.service.UserService;
import com.helei.realtimedatacenter.service.impl.market.RandomMarketRTDataService;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.ConfigurableApplicationContext;

/**
 * 实时数据中心
 */
@SpringBootApplication
public class RealtimeDataCenter {
    public static void main(String[] args) {
        ConfigurableApplicationContext applicationContext = SpringApplication.run(RealtimeDataCenter.class, args);


//        startRTDataStream(applicationContext);

//        updateAllUserInfo2Redis(applicationContext);
//
//        startAccountEventStream(applicationContext);
    }

    /**
     * 开启账户事件流监听
     *
     * @param applicationContext applicationContext
     */
    private static void startAccountEventStream(ConfigurableApplicationContext applicationContext) {
        AccountEventStreamService accountEventStreamService = applicationContext.getBean(AccountEventStreamService.class);
        accountEventStreamService.startAllUserInfoEventStream();
    }

    /**
     * 开启实时数据流
     *
     * @param applicationContext applicationContext
     */
    private static void startRTDataStream(ConfigurableApplicationContext applicationContext) {
        MarketRealtimeDataService marketRealtimeDataService = applicationContext.getBean(RandomMarketRTDataService.class);
        marketRealtimeDataService.startSyncRealTimeKLine();
    }

    /**
     * 更新用户信息到redis
     *
     * @param applicationContext applicationContext
     */
    private static void updateAllUserInfo2Redis(ConfigurableApplicationContext applicationContext) {
        UserService userService = applicationContext.getBean(UserService.class);
        userService.updateAllUserInfo();
    }
}







package com.helei.tradeapplication.cache;

import com.alibaba.fastjson.JSONObject;
import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeType;
import com.helei.dto.account.*;
import com.helei.dto.base.KeyValue;
import com.helei.tradeapplication.config.TradeAppConfig;
import com.helei.tradeapplication.manager.ExecutorServiceManager;
import com.helei.util.RedisKeyUtil;
import lombok.extern.slf4j.Slf4j;
import org.redisson.api.RBucket;
import org.redisson.api.RKeys;
import org.redisson.api.RMap;
import org.redisson.api.RedissonClient;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.function.BiConsumer;


@Slf4j
@Component
public class UserInfoCache {

    private final TradeAppConfig tradeAppConfig = TradeAppConfig.INSTANCE;

    private final ExecutorService executor;


    /**
     * 用户信息缓存， 放基础的数据，不会放实时的仓位和资金信息
     */
    private final ConcurrentMap<String, UserInfo> userInfoCache = new ConcurrentHashMap<>();

    /**
     * 账户信息缓存, account:UserAccountInfo
     */
    private final ConcurrentMap<RunEnv, ConcurrentMap<TradeType, ConcurrentMap<Long, UserAccountInfo>>> accountInfoCache = new ConcurrentHashMap<>();


    @Autowired
    private RedissonClient redissonClient;

    public UserInfoCache(ExecutorServiceManager executorServiceManager) {
        this.executor = executorServiceManager.getQueryExecutor();
    }


    /**
     * 获取账户的实时数据，包含资金和仓位信息
     *
     * @param env       运行环境
     * @param tradeType 交易类型
     * @param userId    用户id
     * @param accountId 账户id
     * @return 实时数据
     */
    public CompletableFuture<UserAccountRealTimeInfo> queryAccountRTInfoFromRedis(RunEnv env, TradeType tradeType, long userId, long accountId) {
        return CompletableFuture.supplyAsync(()->{
            String accountRTDataKey = RedisKeyUtil.getUserAccountEnvRTDataHashKey(env, tradeType, userId);

            RMap<String, String> rtMap = redissonClient.getMap(accountRTDataKey);

            // json手动解析
            JSONObject jsonObject = JSONObject.parseObject(rtMap.get(String.valueOf(accountId)));

            UserAccountRealTimeInfo userAccountRealTimeInfo = jsonObject.toJavaObject(UserAccountRealTimeInfo.class);


            // 解析资金信息
            JSONObject balancesJson = jsonObject.getJSONObject("accountBalanceInfo").getJSONObject("balances");
            List<BalanceInfo> balanceInfos = balancesJson.values().stream().map(o -> ((JSONObject) o).toJavaObject(BalanceInfo.class)).toList();
            userAccountRealTimeInfo.getAccountBalanceInfo().updateBalanceInfos(balanceInfos);


            //解析仓位信息
            JSONObject positionJson = jsonObject.getJSONObject("accountPositionInfo").getJSONObject("positions");
            List<PositionInfo> positionInfos = positionJson.values().stream().map(o -> ((JSONObject) o).toJavaObject(PositionInfo.class)).toList();
            userAccountRealTimeInfo.getAccountPositionInfo().updatePositionInfos(positionInfos);

            return userAccountRealTimeInfo;
        }, executor);
    }

    /**
     * 获取账户的静态数据，包含仓位设置、askey等
     *
     * @param env       运行环境
     * @param tradeType 交易类型
     * @param userId    用户id
     * @param accountId 账户id
     * @return 静态信息
     */
    public CompletableFuture<UserAccountStaticInfo> queryAccountStaticInfoFromRedis(RunEnv env, TradeType tradeType, long userId, long accountId) {
        return CompletableFuture.supplyAsync(()->{
            String staticDataHashKey = RedisKeyUtil.getUserAccountEnvStaticDataHashKey(env, tradeType, userId);

            RMap<String, String> staticMap = redissonClient.getMap(staticDataHashKey);

            return JSONObject.parseObject(staticMap.get(String.valueOf(accountId)), UserAccountStaticInfo.class);
        }, executor);
    }

    /**
     * 从redis查指定环境的所有用户信息
     *
     * @param env       运行环境
     * @param tradeType 交易类型
     */
    public int queryAllUserBaseFromRedis(RunEnv env, TradeType tradeType, BiConsumer<String, UserInfo> consumer) {
        // Step 1 获取用户的pattern
        String accountPattern = RedisKeyUtil.getUserBaseInfoPattern(env, tradeType);
        RKeys keys = redissonClient.getKeys();

        // Step 2 用pattern筛选key， 再查对应key下的UserInfo
        AtomicInteger total = new AtomicInteger();
        keys.getKeysStreamByPattern(accountPattern).forEach(key -> {
            RBucket<String> bucket = redissonClient.getBucket(key);
            UserInfo userInfo = JSONObject.parseObject(bucket.get(), UserInfo.class);

            consumer.accept(key, userInfo);
            total.getAndIncrement();
        });
        return total.get();
    }


    /**
     * 查询用户基础信息，从redis。
     * 不包含账户相关信息
     *
     * @param userRedisKey userRedisKey
     * @return 用户基础信息
     */
    public CompletableFuture<UserInfo> queryUserBaseFromRedis(String userRedisKey) {
        return CompletableFuture.supplyAsync(() -> {

            RBucket<String> bucket = redissonClient.getBucket(userRedisKey);

            return JSONObject.parseObject(bucket.get(), UserInfo.class);
        }, executor);
    }


    /**
     * 查询用户的账户信息，直接写入userInfo参数的对应属性中
     *
     * @param userRedisKey userRedisKey
     * @param userInfo     userInfo
     * @param env          运行环境
     * @param type         交易类型
     */
    public CompletableFuture<UserInfo> queryUserAccountInfoFromRedis(String userRedisKey, UserInfo userInfo, RunEnv env, TradeType type) {
        userInfoCache.put(userRedisKey, userInfo);

        long userId = userInfo.getId();

        userInfo.setAccountInfos(new ArrayList<>());
        List<CompletableFuture<Void>> accountFutures = new ArrayList<>();

        //Step 3 根据账户id从redis查账户信息，并更新map
        for (Long accountId : userInfo.getAccountIds()) {

            //Step 3.1 查询静态信息
            CompletableFuture<UserAccountStaticInfo> staticFuture = queryAccountStaticInfoFromRedis(env, type, userId, accountId);

            //Step 3.2 查询动态信息
            CompletableFuture<UserAccountRealTimeInfo> realtimeFuture = queryAccountRTInfoFromRedis(env, type, userId, accountId);

            //Step 3.3 更新 accountInfoCache
            CompletableFuture<Void> accountFuture = staticFuture.thenAcceptBothAsync(realtimeFuture, (staticInfo, realTimeInfo) -> {

                UserAccountInfo userAccountInfo = accountInfoCache.get(env).get(type).compute(accountId, (k1, v1) -> {
                    if (v1 == null) {
                        v1 = new UserAccountInfo();
                        v1.setUserId(userId);
                        v1.setId(accountId);
                    }
                    v1.setUserAccountStaticInfo(staticInfo);
                    v1.setUserAccountRealTimeInfo(realTimeInfo);

                    return v1;
                });
                userInfo.getAccountInfos().add(userAccountInfo);

                log.info("env[{}]-tradeType[{}]-userId[{}]-accountId[{}]信息同步到cache完成", env, type, userId, accountId);
            }, executor);

            accountFutures.add(accountFuture);
        }

        return CompletableFuture
                .allOf(accountFutures.toArray(new CompletableFuture[0]))
                .thenApplyAsync(Void -> userInfo);
    }

    /**
     * 更新用户账户信息缓存
     *
     * @throws ExecutionException   ExecutionException
     * @throws InterruptedException InterruptedException
     */
    public void updateUserBaseAndRTInfoFromRedis() throws ExecutionException, InterruptedException {
        List<CompletableFuture<Void>> futures = new ArrayList<>();

        /*
         * 获取账户信息，不包括实时的资金信息和仓位信息
         */
        List<KeyValue<RunEnv, TradeType>> runTypeList = tradeAppConfig.getRun_type().getRunTypeList();
        for (KeyValue<RunEnv, TradeType> keyValue : runTypeList) {

            //Step 1 遍历环境
            CompletableFuture<Void> future = CompletableFuture.runAsync(() -> {
                RunEnv env = keyValue.getKey();
                TradeType type = keyValue.getValue();

                accountInfoCache.putIfAbsent(env, new ConcurrentHashMap<>());
                accountInfoCache.get(env).putIfAbsent(type, new ConcurrentHashMap<>());

                log.info("开始初始化环境env[{}]-tradeType[{}]的用户信息", env, type);

                //Step 2 从redis查询UserBaseInfo
                List<CompletableFuture<UserInfo>> accountFutures = new ArrayList<>();
                int total = queryAllUserBaseFromRedis(env, type, (k, userInfo) -> {

                    //Step 3 再从redis查询用户账户相关信息
                    CompletableFuture<UserInfo> accountFuture = queryUserAccountInfoFromRedis(k, userInfo, env, type);
                    accountFutures.add(accountFuture);
                });

                try {
                    CompletableFuture
                            .allOf(accountFutures.toArray(new CompletableFuture[0]))
                            .whenCompleteAsync((unused, throwable) -> {
                                if (throwable != null) {
                                    log.error("获取env[{}]-tradeType[{}]的用户信息出错", env, type, throwable);
                                } else {
                                    log.info("环境env[{}]-tradeType[{}]的用户信息初始化完毕, 共[{}]个用户", env, type, total);
                                }
                            })
                            .get();
                } catch (InterruptedException | ExecutionException e) {
                    throw new RuntimeException(e);
                }
            }, executor);

            futures.add(future);
        }

        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
                .whenCompleteAsync((unused, throwable) -> {
                    if (throwable != null) {
                        log.error("更新用户信息时发生错误", throwable);
                        System.exit(-1);
                    }
                    log.info("所有运行环境的用户信息初始化完毕, 环境列表: [{}]", runTypeList);
                }).get();
    }

    /**
     * 从redis查询用户信息
     * 查询出的用户信息会从新放入缓存
     *
     * @param env       运行环境
     * @param tradeType 交易类型
     * @param userId    用户id
     * @return CompletableFuture<UserInfo>
     */
    public CompletableFuture<UserInfo> queryUserInfoFromRedis(RunEnv env, TradeType tradeType, long userId) {
        String key = RedisKeyUtil.getUserBaseInfoKey(env, tradeType, userId);

        return queryUserBaseFromRedis(key)
                .thenApplyAsync(userInfo -> {
                    CompletableFuture<UserInfo> future = queryUserAccountInfoFromRedis(key, userInfo, env, tradeType);
                    try {
                        return future.get();
                    } catch (InterruptedException | ExecutionException e) {
                        throw new RuntimeException(String.format("查询[%s]-[%s}-[%s]的redis账户数据出错", env.name(), tradeType.name(), userId), e);
                    }
                });
    }


    /**
     * 从cache获取账户信息
     *
     * @param env       env
     * @param tradeType tradeType
     * @param accountId accountId
     * @return UserAccountInfo
     */
    public UserAccountInfo queryAccountInfoFromCache(RunEnv env, TradeType tradeType, long accountId) {
        ConcurrentMap<TradeType, ConcurrentMap<Long, UserAccountInfo>> map1 = accountInfoCache.get(env);
        if (map1 == null) return null;

        ConcurrentMap<Long, UserAccountInfo> map2 = map1.get(tradeType);
        if (map2 == null) return null;

        return map2.get(accountId);
    }

    /**
     * 从本地缓存中查询指定环境的用户信息
     *
     * @param env       运行环境
     * @param tradeType 交易类型
     * @return List<UserInfo>
     */
    public List<UserAccountInfo> queryAllAccountInfoFromCache(RunEnv env, TradeType tradeType) {
        ConcurrentMap<TradeType, ConcurrentMap<Long, UserAccountInfo>> map1 = accountInfoCache.get(env);
        if (map1 != null) {
            ConcurrentMap<Long, UserAccountInfo> map2 = map1.get(tradeType);
            if (map2 != null) {
                return map2.values().stream().toList();
            }
        }
        return Collections.emptyList();
    }


}




package com.helei.tradeapplication.listener;

import cn.hutool.core.util.StrUtil;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.jetbrains.annotations.NotNull;
import org.springframework.kafka.listener.MessageListener;
import org.springframework.kafka.support.Acknowledgment;

import java.util.concurrent.ExecutorService;


@Slf4j
public abstract class KafkaTopicListener<R> implements MessageListener<String, String> {

    private final ExecutorService executor;

    protected KafkaTopicListener(ExecutorService executor) {
        this.executor = executor;
    }

    @Override
    public void onMessage(@NotNull ConsumerRecord<String, String> record) {
        executor.execute(()->{
            String topic = record.topic();
            String value = record.value();
            log.info("topic[{}]收到消息[{}]", topic, value);

            if (StrUtil.isBlank(value)) {
                log.warn("receive null kafka trade signal, topic[{}] key [{}]", topic, record.key());
                return;
            }

            try {
                invoke(topic, convertJsonToTarget(value));
            } catch (Exception e) {
                log.error("处理kafka topic[{}] 消息[{}]时出错", topic, value, e);
            }
        });
    }

    @Override
    public void onMessage(@NotNull ConsumerRecord<String, String> record, Acknowledgment acknowledgment) {
        executor.execute(()->{
            String topic = record.topic();
            String value = record.value();
            log.info("topic[{}]收到消息[{}]", topic, value);
            if (StrUtil.isBlank(value)) {
                log.warn("receive null kafka trade signal, topic[{}] key [{}]", topic, record.key());
                return;
            }

            try {
                R r = convertJsonToTarget(value);

                if(invoke(topic, r)){
                    acknowledgment.acknowledge();
                }
            } catch (Exception e) {
                log.error("处理kafka topic[{}] 消息[{}]时出错", topic, value, e);
            }
        });
    }


    public abstract R convertJsonToTarget(String json);


    public abstract boolean invoke(String topic, R message);
}


package com.helei.tradeapplication.service.impl;

import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeType;
import com.helei.tradeapplication.config.TradeAppConfig;
import com.helei.tradeapplication.listener.KafkaTradeSignalListener;
import com.helei.tradeapplication.manager.ExecutorServiceManager;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.listener.ConcurrentMessageListenerContainer;
import org.springframework.kafka.listener.MessageListener;
import org.springframework.stereotype.Service;

import java.util.List;


@Slf4j
@Service
public class KafkaConsumerService {

    private final TradeAppConfig tradeAppConfig = TradeAppConfig.INSTANCE;

    @Autowired
    private ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory;

    @Autowired
    private ExecutorServiceManager executorServiceManager;

    @Autowired
    private KafkaTradeSignalService tradeSignalService;

    /**
     * 开始交易信号消费
     *
     * @param env       运行环境
     * @param tradeType 交易类型
     */
    public void startTradeSignalConsumer(RunEnv env, TradeType tradeType) {

        tradeAppConfig.getSignalTopics(env, tradeType, (prefix, signalNames) -> {
            if (signalNames.isEmpty()) {
                log.warn("没有配置env[{}]-tradeType[{}]类型的交易信号topic", env, tradeType);
                return;
            }
            log.info("注册监听topic [{}*] signalNames[{}]交易信号 ", prefix, signalNames);
            List<String> topics = signalNames.stream().map(name -> (prefix + name).toLowerCase()).toList();
            startConsumer(topics, new KafkaTradeSignalListener(env, tradeType, tradeSignalService, executorServiceManager.getTradeSignalResolveExecutor()));
        });
    }


    /**
     * 开始kafka消费
     *
     * @param topics          topics
     * @param messageListener messageListener
     */
    public void startConsumer(List<String> topics, MessageListener<String, String> messageListener) {
        ConcurrentMessageListenerContainer<String, String> container = kafkaListenerContainerFactory.createContainer(topics.toArray(new String[0]));

        container.setupMessageListener(messageListener);
        container.start();
    }
}



package com.helei.tradeapplication.service.impl;


import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeType;
import com.helei.dto.account.UserAccountInfo;
import com.helei.dto.account.UserAccountStaticInfo;
import com.helei.tradeapplication.dto.GroupOrder;
import com.helei.dto.trade.TradeSignal;
import com.helei.interfaces.CompleteInvocation;
import com.helei.tradeapplication.manager.ExecutorServiceManager;
import com.helei.tradeapplication.service.OrderService;
import com.helei.tradeapplication.service.TradeSignalService;
import com.helei.tradeapplication.service.UserAccountInfoService;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;


/**
 * 处理交易信号
 */
@Slf4j
@Service
public class KafkaTradeSignalService implements TradeSignalService {

    private final ExecutorService executor;

    @Autowired
    private UserAccountInfoService userAccountInfoService;

    @Autowired
    private OrderService orderService;


    public KafkaTradeSignalService(ExecutorServiceManager executorServiceManager) {
        this.executor = executorServiceManager.getTradeExecutor();
    }


    /**
     * 处理交易信号
     *
     * @param runEnv    运行环境
     * @param tradeType 交易类型
     * @param signal    信号
     * @return true 无论处理结果如何都忽略到改信号
     */
    public boolean resolveTradeSignal(RunEnv runEnv, TradeType tradeType, TradeSignal signal) {

        try {
            userAccountInfoService
                    // 1 查询环境下的账户
                    .queryEnvAccountInfo(runEnv, tradeType)
                    // 2 生成订单并交易
                    .thenApplyAsync(accounts -> makeOrdersAndSend2Trade(runEnv, tradeType, signal, accounts), executor);
        } catch (Exception e) {
            log.error("处理信号[{}]时发生错误", signal, e);
        }

        return true;
    }


    /**
     * 构建订单，符合条件的提交到交易.
     * <p>并不会真正把订单提交到交易所，而是写入数据库后，再写入kafka的topic里</p>
     *
     * @param runEnv       runEnv
     * @param tradeType    tradeType
     * @param signal       signal
     * @param accountInfos accountInfos
     * @return List<BaseOrder>
     */
    private List<GroupOrder> makeOrdersAndSend2Trade(RunEnv runEnv, TradeType tradeType, TradeSignal signal, List<UserAccountInfo> accountInfos) {

        List<CompletableFuture<GroupOrder>> futures = new ArrayList<>();

        for (UserAccountInfo accountInfo : accountInfos) {
            long userId = accountInfo.getUserId();
            long accountId = accountInfo.getId();

            //Step 1 过滤掉账户设置不接受此信号的
            if (filterAccount(runEnv, tradeType, signal, accountInfo.getUserAccountStaticInfo())) {
                log.warn("accountId[{}]不能执行信号 [{}]", accountId, signal);
                continue;
            }

            CompletableFuture<GroupOrder> future = userAccountInfoService
                    //Step 2 查询实时的账户数据
                    .queryAccountNewInfo(runEnv, tradeType, userId, accountId)
                    //Step 3 生产订单
                    .thenApplyAsync(newAccountInfo -> {
                        final GroupOrder[] groupOrder = {null};

                        try {
                            CountDownLatch latch = new CountDownLatch(1);

                            orderService.makeOrder(newAccountInfo, signal, new CompleteInvocation<>() {
                                @Override
                                public void success(GroupOrder order) {
                                    groupOrder[0] = order;
                                    log.info("创建订单[{}]成功", order);
                                }

                                @Override
                                public void fail(GroupOrder order, String errorMsg) {
                                    groupOrder[0] = order;
                                    log.info("创建订单失败[{}],错误原因[{}]", order, errorMsg);
                                }

                                @Override
                                public void finish() {
                                    latch.countDown();
                                }
                            });

                            //等待订单创建完成
                            latch.await();

                        } catch (Exception e) {
                            log.error("为accountId[{}]创建订单时出错, signal[{}]", accountId, signal, e);
                        }
                        return groupOrder[0];
                    })
                    .exceptionallyAsync(throwable -> {
                        if (throwable != null) {
                            log.error("创建订单时发生错误", throwable);
                        }
                        return null;
                    });

            futures.add(future);
        }


        //等待执行完成
        List<GroupOrder> groupOrders = new ArrayList<>();
        for (CompletableFuture<GroupOrder> future : futures) {
            try {
                GroupOrder order = future.get();
                groupOrders.add(order);
            } catch (ExecutionException | InterruptedException e) {
                log.error("获取订单结果处理订单结果出错", e);
                throw new RuntimeException(e);
            }
        }
        return groupOrders;
    }


    /**
     * 根据账户设置过滤
     *
     * @param signal  信号
     * @param staticInfo 账户静态信息
     * @return List<UserAccountInfo>
     */
    private boolean filterAccount(RunEnv runEnv, TradeType tradeType, TradeSignal signal, UserAccountStaticInfo staticInfo) {
        return !staticInfo.isUsable() ||
                !staticInfo.getSubscribeSymbol().contains(signal.getSymbol().toLowerCase())||
                !runEnv.equals(staticInfo.getRunEnv()) ||
                !tradeType.equals(staticInfo.getTradeType());
    }
}



package com.helei.tradeapplication.service.impl;

import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeType;
import com.helei.dto.account.UserAccountInfo;
import com.helei.tradeapplication.cache.UserInfoCache;
import com.helei.tradeapplication.manager.ExecutorServiceManager;
import com.helei.tradeapplication.service.UserAccountInfoService;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;

import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;

@Slf4j
@Component
public class UserAccountInfoServiceImpl implements UserAccountInfoService {

    private final ExecutorService executor;

    @Autowired
    private UserInfoCache userInfoCache;

    public UserAccountInfoServiceImpl(ExecutorServiceManager executorServiceManager) {
        this.executor = executorServiceManager.getQueryExecutor();
    }

    @Override
    public CompletableFuture<List<UserAccountInfo>> queryEnvAccountInfo(RunEnv env, TradeType tradeType) {
        return CompletableFuture.supplyAsync(() -> userInfoCache.queryAllAccountInfoFromCache(env, tradeType), executor);
    }

    @Override
    public CompletableFuture<UserAccountInfo> queryAccountNewInfo(RunEnv env, TradeType tradeType, long userId, long accountId) {
        //Step 1 从缓存拿基础信息
        UserAccountInfo userAccountInfo = userInfoCache.queryAccountInfoFromCache(env, tradeType, accountId);

        if (userAccountInfo == null) {
            log.warn("ent[{}]-tradeType[{}]-accountId[{}] 从缓存获取账户信息失败", env, tradeType, accountId);
            //从redis拿一次
            return userInfoCache.queryUserInfoFromRedis(env, tradeType, userId)
                    .thenApplyAsync(userInfo -> userInfoCache.queryAccountInfoFromCache(env, tradeType, accountId));
        } else {
            //Step 2 从redis拿实时信息
            return userInfoCache.queryAccountRTInfoFromRedis(env, tradeType, userId, accountId).thenApplyAsync(rtInfo -> {
                userAccountInfo.setUserAccountRealTimeInfo(rtInfo);
                return userAccountInfo;
            });
        }
    }
}







package com.helei.tradeapplication.service;

import com.helei.constants.order.OrderEvent;
import com.helei.constants.order.GroupOrderStatus;
import com.helei.tradeapplication.dto.GroupOrder;
import com.helei.interfaces.CompleteInvocation;
import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.extern.slf4j.Slf4j;

import java.util.concurrent.*;


@Slf4j
public abstract class OrderEventProcessService implements OrderService {


    /**
     * 写入db的重试次数
     */
    private static final int WRITE_DB_RETRY_TIMES = 3;

    /**
     * 写入kafka的重试次数
     */
    private static final int WRITE_KAFKA_RETRY_TIMES = 3;


    /**
     * 阻塞队列， 用于存放订单和当前订单的事件
     */
    private final BlockingQueue<OrderProcessTask> eventQueue = new LinkedBlockingQueue<>();


    /**
     * 存放订单回调的map
     */
    private final ConcurrentMap<GroupOrder, CompleteInvocation<GroupOrder>> invocationMap = new ConcurrentHashMap<>();


    /**
     * 记录重试次数的map
     */
    private final ConcurrentMap<GroupOrder, Integer> retryMap = new ConcurrentHashMap<>();


    /**
     * 执行的线程池
     */
    private final ExecutorService executor;


    public OrderEventProcessService(ExecutorService executor) {
        this.executor = executor;
    }


    /**
     * 提交订单事件
     *
     * @param order              订单
     * @param event              订单事件
     * @param completeInvocation 完成的回调函数
     */
    public void submitOrderEvent(GroupOrder order, OrderEvent event, CompleteInvocation<GroupOrder> completeInvocation) {
        invocationMap.compute(order, (k, v) -> {
            submitOrderEvent(order, event);
            return completeInvocation;
        });
    }

    /**
     * 提交订单事件
     *
     * @param order 订单
     * @param event 订单事件
     */
    public void submitOrderEvent(GroupOrder order, OrderEvent event) {
        try {
            eventQueue.put(new OrderProcessTask(order, event));
        } catch (InterruptedException e) {
            log.error("提交订单[{}]事件[{}]失败", order, event, e);
            throw new RuntimeException("提交订单事件失败", e);
        }
    }

    /**
     * 事件处理
     *
     * @param order 订单
     * @param event 事件
     */
    public void processOrderEvent(GroupOrder order, OrderEvent event) {
        log.debug("开始处理订单[{}]的事件[{}]", order, event);

        OrderEvent next = switch (event) {
            case CREATED_ORDER -> createdOrderProcess(order);

            case SEND_TO_DB -> sendToDBProcess(order);
            case SEND_TO_KAFKA -> sendToKafkaProcess(order);

            case SEND_TO_DB_RETRY -> sendToDBRetryProcess(order);
            case SEND_TO_KAFKA_RETRY -> sendToKafkaRetryProcess(order);

            case SEND_TO_DB_FINAL_ERROR -> errorProcess(order, OrderEvent.SEND_TO_DB_FINAL_ERROR);
            case SEND_TO_KAFKA_FINAL_ERROR -> errorProcess(order, OrderEvent.SEND_TO_KAFKA_FINAL_ERROR);
            case UN_SUPPORT_EVENT_ERROR -> errorProcess(order, OrderEvent.UN_SUPPORT_EVENT_ERROR);

            case BALANCE_INSUFFICIENT -> balanceInsufficientProcess(order);

            case COMPLETE -> successProcess(order);
            case ERROR -> errorProcess(order, OrderEvent.ERROR);
            case CANCEL -> cancelProcess(order);
        };

        if (next != null) {
            submitOrderEvent(order, next);
        }

        log.debug("订单[{}]的事件[{}]处理完毕", order, event);
    }


    /**
     * 资金不足的订单处理
     *
     * @param order groupOrder
     * @return 下一个事件
     */
    private OrderEvent balanceInsufficientProcess(GroupOrder order) {
        if (GroupOrderStatus.BALANCE_INSUFFICIENT.equals(order.getGroupOrderStatus())) {
            try {
                //写入数据库
                writeOrder2DB(order);
            } catch (Exception e) {
                //重试
                return OrderEvent.SEND_TO_DB_RETRY;
            }
            //错误事件
            errorProcess(order, OrderEvent.BALANCE_INSUFFICIENT);
        }
        return OrderEvent.UN_SUPPORT_EVENT_ERROR;
    }


    /**
     * 取消订单
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent cancelProcess(GroupOrder order) {
        //TODO 取消订单逻辑，未写入kafka的标记就好，写入kafka的还需要向另外的kafka里写上取消的消息，订单提交服务收到后进行取消

        log.warn("取消订单 [{}]", order);
        return null;
    }


    /**
     * 执行成功的事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent successProcess(GroupOrder order) {

        CompleteInvocation<GroupOrder> invocation = invocationMap.remove(order);

        if (invocation != null) {
            invocation.success(order);
            invocation.finish();
        }

        return null;
    }


    /**
     * 错误事件处理
     *
     * @param order order
     * @param event 时间
     * @return 下一个事件
     */
    private OrderEvent errorProcess(GroupOrder order, OrderEvent event) {

        CompleteInvocation<GroupOrder> invocation = invocationMap.remove(order);

        if (invocation != null) {
            invocation.fail(order, event.name());
            invocation.finish();
        }

        return null;
    }


    /**
     * 发送到kafka错误重试事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent sendToKafkaRetryProcess(GroupOrder order) {
        if (GroupOrderStatus.WRITE_IN_KAFKA.equals(order.getGroupOrderStatus())) {
            Integer times = retryMap.remove(order);
            times = times == null ? 0 : times;

            //超过重试次数
            if (times > WRITE_KAFKA_RETRY_TIMES) {
                return OrderEvent.SEND_TO_KAFKA_FINAL_ERROR;
            }


            try {
                GroupOrder result = writeOrder2Kafka(order);

                if (result == null) return OrderEvent.CANCEL;

                return OrderEvent.COMPLETE;
            } catch (Exception e) {
                log.error("写入Order[{}]到kafka发生错误,重试次数[{}]", order, times, e);
                retryMap.put(order, times + 1);
                return OrderEvent.SEND_TO_KAFKA_RETRY;
            }
        }

        return OrderEvent.UN_SUPPORT_EVENT_ERROR;
    }


    /**
     * 发送到kafka事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent sendToKafkaProcess(GroupOrder order) {
        if (GroupOrderStatus.WRITE_IN_DB.equals(order.getGroupOrderStatus())) {
            // 发送kafka
            try {
                order.setGroupOrderStatus(GroupOrderStatus.WRITE_IN_KAFKA);

                GroupOrder result = writeOrder2Kafka(order);

                if (result == null) return OrderEvent.CANCEL;
            } catch (Exception e) {
                log.error("写入Order[{}]到kafka发生错误", order, e);
                return OrderEvent.SEND_TO_KAFKA_RETRY;
            }
            return OrderEvent.COMPLETE;
        }
        return OrderEvent.UN_SUPPORT_EVENT_ERROR;
    }


    /**
     * 发送到DB错误重试事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent sendToDBRetryProcess(GroupOrder order) {
        GroupOrderStatus groupOrderStatus = order.getGroupOrderStatus();
        if (GroupOrderStatus.WRITE_IN_DB.equals(groupOrderStatus) || GroupOrderStatus.BALANCE_INSUFFICIENT.equals(groupOrderStatus)) {
            Integer times = retryMap.remove(order);
            times = times == null ? 0 : times;

            //超过重试次数
            if (times > WRITE_DB_RETRY_TIMES) {
                return OrderEvent.SEND_TO_DB_FINAL_ERROR;
            }

            try {
                GroupOrder result = writeOrder2DB(order);

                if (result == null) return OrderEvent.CANCEL;

                //资金不足，只写入数据库记录
                if (GroupOrderStatus.BALANCE_INSUFFICIENT.equals(groupOrderStatus)) return OrderEvent.ERROR;

                return OrderEvent.SEND_TO_KAFKA;
            } catch (Exception e) {
                log.error("写入Order[{}]到数据库发生错误, 重试次数[{}]", order, times, e);
                retryMap.put(order, times + 1);
                return OrderEvent.SEND_TO_DB_RETRY;
            }
        }

        return OrderEvent.UN_SUPPORT_EVENT_ERROR;
    }


    /**
     * 发送到DB事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent sendToDBProcess(GroupOrder order) {
        if (GroupOrderStatus.CREATED.equals(order.getGroupOrderStatus())) {
            // 写数据库
            try {
                order.setGroupOrderStatus(GroupOrderStatus.WRITE_IN_DB);

                GroupOrder result = writeOrder2DB(order);

                if (result == null) return OrderEvent.CANCEL;
            } catch (Exception e) {
                log.error("写入Order[{}]到数据库发生错误", order, e);
                return OrderEvent.SEND_TO_DB_RETRY;
            }
            return OrderEvent.SEND_TO_KAFKA;
        }
        return OrderEvent.UN_SUPPORT_EVENT_ERROR;
    }

    /**
     * 创建订单事件处理
     *
     * @param order order
     * @return 下一个事件
     */
    private OrderEvent createdOrderProcess(GroupOrder order) {
        //订单创建事件
        order.setGroupOrderStatus(GroupOrderStatus.CREATED);
        return OrderEvent.SEND_TO_DB;
    }


    /**
     * 开始处理事件
     */
    public void startProcessEvents() {
        while (!eventQueue.isEmpty()) {
            try {
                OrderProcessTask task = eventQueue.take();

                executor.execute(() -> processOrderEvent(task.getOrder(), task.getOrderEvent()));
            } catch (InterruptedException e) {
                log.error("处理事件时发生错误", e);
            }
        }
    }


    /**
     * 订单处理任务，包含订单信息和订单事件
     */
    @Data
    @AllArgsConstructor
    @NoArgsConstructor
    public static class OrderProcessTask {

        /**
         * 订单信息
         */
        private GroupOrder order;

        /**
         * 订单事件
         */
        private OrderEvent orderEvent;
    }


}



package com.helei.tradeapplication.supporter;

import com.helei.constants.order.OrderType;
import com.helei.constants.order.TimeInForce;
import com.helei.dto.account.*;
import com.helei.dto.order.BaseOrder;
import com.helei.dto.order.CEXTradeOrder;
import com.helei.dto.order.type.*;
import com.helei.dto.trade.TradeSignal;
import com.helei.snowflack.BRStyle;
import com.helei.snowflack.SnowFlakeFactory;
import com.helei.util.OrderQuantityCalUtil;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Component;

import java.math.BigDecimal;


/**
 * 构建用于交易的订单，自带id
 */
@Component
public class TradeOrderBuildSupporter {

    @Autowired
    private SnowFlakeFactory snowFlakeFactory;

    /**
     * 构建限价单
     *
     * @param accountInfo   账户信息
     * @param signal        信号
     * @return 限价单
     */
    public LimitOrder buildLimitOrder(UserAccountInfo accountInfo, TradeSignal signal) {
        // Step 1 参数检验
        UserAccountStaticInfo staticInfo = accountInfo.getUserAccountStaticInfo();
        UserAccountRealTimeInfo realTimeInfo = accountInfo.getUserAccountRealTimeInfo();

        String quote = staticInfo.getQuote();


        BigDecimal enterPrice = signal.getEnterPrice();
        if (enterPrice == null) return null;

        AccountPositionConfig positionConfig = staticInfo.getAccountPositionConfig();

        // Step 2 创建订单
        String symbol = signal.getSymbol();


        // Step 2.1 基础订单信息
        BaseOrder baseOrder = BaseOrder.builder()
                .orderId(nextId(OrderType.LIMIT))
                .runEnv(staticInfo.getRunEnv())
                .tradeType(staticInfo.getTradeType())
                .cexType(staticInfo.getCexType())
                .symbol(symbol)
                .side(signal.getTradeSide())
                .positionSide(positionConfig.getPositionSide())
                .userId(accountInfo.getUserId())
                .accountId(staticInfo.getId())
                .build();
        LimitOrder limitOrder = new LimitOrder(baseOrder);

        // Step 2.2 订单价格、数量等 LimitOrder 信息
        BalanceInfo balanceInfo = realTimeInfo.getAccountBalanceInfo().getBalances().get(quote);
        PositionInfo positionInfo = realTimeInfo.getAccountPositionInfo().getPositions().get(symbol.toUpperCase());


        BigDecimal quantity = OrderQuantityCalUtil.riskPercentBasedQuantityCalculate(
                balanceInfo.getAvailableBalance(),
                BigDecimal.valueOf(positionConfig.getRiskPercent()),
                enterPrice,
                positionInfo.getEntryPrice(),
                positionInfo.getPositionAmt(),
                signal.getStopPrice()
        );

        limitOrder.setTimeInForce(TimeInForce.GTC);
        limitOrder.setPrice(enterPrice);
        limitOrder.setQuantity(quantity);

        return limitOrder;
    }


    /**
     * 构建市价单
     *
     * @param accountInfo   账户信息
     * @param signal        信号
     * @return 限价单
     */
    public MarketOrder buildMarketOrder(UserAccountInfo accountInfo, TradeSignal signal) {
        return null;
    }

    /**
     * 构建市价止损单
     *
     * @param accountInfo   账户信息
     * @param symbol        交易对
     * @return 限价单
     */
    public StopLossMarketOrder buildStopMarketOrder(UserAccountInfo accountInfo, String symbol) {
        return null;
    }


    /**
     * 构建限价止损单
     *
     * @param accountInfo   账户信息
     * @param symbol        交易对
     * @return 限价单
     */
    public StopLossLimitOrder buildStopLimitOrder(UserAccountRealTimeInfo accountInfo, String symbol) {
        return null;
    }

    /**
     * 构建市价止盈单
     *
     * @param accountInfo   账户信息
     * @param symbol        交易对
     * @return 限价单
     */
    public TakeProfitMarketOrder buildTakeProfitMarketOrder(UserAccountRealTimeInfo accountInfo, String symbol) {
        return null;
    }

    /**
     * 构建限价止盈单
     *
     * @param accountInfo   账户信息
     * @param symbol        交易对
     * @return 限价单
     */
    public TakeProfitLimitOrder buildTakeProfitLimitOrder(UserAccountRealTimeInfo accountInfo, String symbol) {
        return null;
    }

    /**
     * buildTrailingSTIDMarketOrder
     *
     * @param accountInfo   账户信息
     * @param symbol        交易对
     * @return 限价单
     */
    public CEXTradeOrder buildTrailingSTIDMarketOrder(UserAccountRealTimeInfo accountInfo, String symbol) {
        return null;
    }


    /**
     * 获取下一id
     * @param orderType 订单类型
     * @return 订单id
     */
    private String nextId(OrderType orderType) {
        return snowFlakeFactory.nextId(BRStyle.TRADE_SIGNAL, orderType.name());
    }
}




package com.helei.tradeapplication;


import com.helei.constants.RunEnv;
import com.helei.constants.trade.TradeType;
import com.helei.dto.base.KeyValue;
import com.helei.tradeapplication.cache.UserInfoCache;
import com.helei.tradeapplication.config.TradeAppConfig;
import com.helei.tradeapplication.service.impl.KafkaConsumerService;
import lombok.extern.slf4j.Slf4j;
import org.mybatis.spring.annotation.MapperScan;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.ConfigurableApplicationContext;

import java.util.concurrent.ExecutionException;

@Slf4j
@SpringBootApplication
@MapperScan("com.helei.tradeapplication.mapper")
public class TradeApplication {

    private static TradeAppConfig tradeAppConfig = TradeAppConfig.INSTANCE;

    public static void main(String[] args) {
        ConfigurableApplicationContext applicationContext = SpringApplication.run(TradeApplication.class, args);


        initAllUserInfo(applicationContext);


        startTradeSignalConsumer(applicationContext);

    }

    /**
     * 开启交易信号消费
     *
     * @param applicationContext applicationContext
     */
    private static void startTradeSignalConsumer(ConfigurableApplicationContext applicationContext) {
        KafkaConsumerService kafkaConsumerService = applicationContext.getBean(KafkaConsumerService.class);
        for (KeyValue<RunEnv, TradeType> keyValue : tradeAppConfig.getRun_type().getRunTypeList()) {
            kafkaConsumerService.startTradeSignalConsumer(keyValue.getKey(), keyValue.getValue());
        }
    }


    /**
     * 初始化用户信息
     *
     * @param applicationContext applicationContext
     */
    private static void initAllUserInfo(ConfigurableApplicationContext applicationContext) {
        UserInfoCache userInfoCache = applicationContext.getBean(UserInfoCache.class);
        try {
            userInfoCache.updateUserBaseAndRTInfoFromRedis();
        } catch (ExecutionException | InterruptedException e) {
            throw new RuntimeException(e);
        }
    }
}





